{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17e7de16-537a-41df-ab2c-b6050c8ca08d",
   "metadata": {},
   "source": [
    "# L01_E04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3bece6-1697-41e2-91c7-856e3e9b77a8",
   "metadata": {},
   "source": [
    "We saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cab2e2-3933-4d5b-8e8f-2e3de418862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16811fba-1c56-45b7-a676-a37a2f94ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "words = open('../names.txt','r').read().splitlines()\n",
    "\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "words_tr = words[:n1]\n",
    "words_dev = words[n1:n2]\n",
    "words_te = words[n2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b4a543-c9a0-44b3-be46-fa7fb5c30e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words_tr))))\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6217d-e93d-49de-ab02-7513ad00f17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctoi = {c : i+1 for i,c in enumerate(chars)}\n",
    "ctoi['.'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a7f53-1e28-411c-8368-77158b211881",
   "metadata": {},
   "outputs": [],
   "source": [
    "itoc = {i:c for c,i in ctoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f5d4b-b9de-4210-b067-be14339aef17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_chars = len(ctoi.keys())\n",
    "num_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181bf069-91c9-4d76-ad46-2ef79281edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {}\n",
    "for i0,c0 in sorted(itoc.items(), key=lambda kv: kv[0]):\n",
    "    for i1,c1 in sorted(itoc.items(), key=lambda kv: kv[0]):\n",
    "        #print((i0*num_chars) + i1,c0,c1)\n",
    "        stoi[(c0,c1)] = (i0*num_chars) + i1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c10136-33e8-456d-9af9-d9ec45afe26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    xs,ys = [],[]\n",
    "    \n",
    "    for word in words:\n",
    "        chs = '..' + word + '.'\n",
    "        for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "            ix1 = stoi[ch1,ch2]\n",
    "            ix2 = ctoi[ch3]\n",
    "            xs.append(ix1)\n",
    "            ys.append(ix2)\n",
    "    \n",
    "    # prefer to use torch.tensor instead of torch.Tensor\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    num = xs.nelement()\n",
    "    print(f'number of examples: {num}')   \n",
    "\n",
    "    return xs, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d76cb-179f-4cce-b5d1-198bc8fb879f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: 182625\n",
      "number of examples: 22655\n",
      "number of examples: 22866\n"
     ]
    }
   ],
   "source": [
    "Xtr,Ytr=build_dataset(words_tr)\n",
    "Xdev,Ydev=build_dataset(words_dev)\n",
    "Xte,Yte=build_dataset(words_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8575576-403f-4023-8b88-811a7618fead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74032027-362d-4606-86a2-98e4a6d3eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d9d5c3-ef63-4c52-a165-7f0df9f2a4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2389516830444336 0.006938230711966753 2.2389516830444336\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((num_chars*num_chars,num_chars), generator=g, requires_grad=True) #single layer of 27 neurons each getting 27x27 inputs\n",
    "\n",
    "reg = 0.004552933843974289\n",
    "for k in range(400):\n",
    "    xs, ys = Xtr, Ytr\n",
    "    logits = W[xs] #log-counts\n",
    "    counts = logits.exp() # exponentiate the logits to get fake counts\n",
    "    probs = counts/counts.sum(1,keepdims=True)  \n",
    "    \n",
    "    loss_1 = (-(probs[torch.arange(xs.nelement()),ys]).log()).mean()\n",
    "    loss_2 = reg*(W**2).mean() #regularization loss\n",
    "    \n",
    "    loss = loss_1 #+ loss_2\n",
    "\n",
    "    # if k%40==0: print(loss.item())\n",
    "    \n",
    "    #backward pass\n",
    "    W.grad = None #More efficient than setting to zero directly. Lack of gradient is interpreted as zero by PyTorch\n",
    "    loss.backward()\n",
    "    \n",
    "    #update\n",
    "    W.data += -4*50 * W.grad\n",
    "print(loss_1.item(), loss_2.item(), loss.item())    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc12040-8de4-4e0c-8a18-0f36e3d9b599",
   "metadata": {},
   "source": [
    "Finally let's evaluate the loss on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab46fc72-f6c3-48ba-8d95-972cb5ada582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.323352575302124\n"
     ]
    }
   ],
   "source": [
    "xs, ys = Xte, Yte\n",
    "logits = W[xs] #log-counts\n",
    "counts = logits.exp() # exponentiate the logits to get fake counts\n",
    "probs = counts/counts.sum(1,keepdims=True)\n",
    "loss = (-(probs[torch.arange(xs.nelement()),ys]).log()).mean()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6484a4-4b79-4254-aad8-b02793d062a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
