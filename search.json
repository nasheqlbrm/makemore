[
  {
    "objectID": "build_makemore_yay.html",
    "href": "build_makemore_yay.html",
    "title": "Build makemore yay",
    "section": "",
    "text": "words = open('../names.txt').read().splitlines()\nlen(words)\n\n32033\nwords[:5]\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia']\nThe length of the smallest and largest word\nmin(map(len, words)), max(map(len, words))\n\n(2, 15)\nAn ostensibly small entry such as isabella packs quite a few examples to learn from:\nSo the fact that we have \\(32,033\\) of them means we have quite a lot of structure to model.",
    "crumbs": [
      "Build makemore yay"
    ]
  },
  {
    "objectID": "build_makemore_yay.html#train-a-bigram-model",
    "href": "build_makemore_yay.html#train-a-bigram-model",
    "title": "Build makemore yay",
    "section": "Train a bigram model",
    "text": "Train a bigram model\nInstantiate a \\(27\\) by \\(27\\) array for holding the counts. \\(27\\) because there are 26 characters in the English alphabet and we add an additional token to represent the start or end of the sequence.\n\nN = torch.zeros((27,27), \n                dtype=torch.int32 # since this will hold counts\n               )\n\n\nchars = sorted(list(set(''.join(words))))\nchars[:3], len(chars)\n\n(['a', 'b', 'c'], 26)\n\n\n\nstoi = {s : i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\n\n\nitos = {v : k for k,v in stoi.items()}\n\n\nfor w in words:\n    chs = ['.'] + list(w) + ['.']\n    for ch1,ch2 in zip(chs,chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        N[ix1,ix2] += 1\n\n\nimport matplotlib.pyplot as plt\n\n\nN[:3,:3]\n\ntensor([[   0, 4410, 1306],\n        [6640,  556,  541],\n        [ 114,  321,   38]], dtype=torch.int32)\n\n\n\nplt.figure(figsize=(16,16))\nplt.imshow(N, cmap='Blues')\nfor i in range(27):\n    for j in range(27):\n        chstr = itos[i] + itos[j]\n        plt.text(j, i, chstr, ha='center', va='bottom', color='gray')\n        plt.text(j, i, N[i,j].item(), ha='center', va='top', color='gray')\nplt.axis('off');",
    "crumbs": [
      "Build makemore yay"
    ]
  },
  {
    "objectID": "build_makemore_yay.html#sample-from-bigram-model",
    "href": "build_makemore_yay.html#sample-from-bigram-model",
    "title": "Build makemore yay",
    "section": "Sample from bigram model",
    "text": "Sample from bigram model\nNormalize N so that each row becomes a probability distribution. Now P summarizes the statistics of these sequences. * Add a 1 to avoid a zero count situation.\n\nP = (N+1).float()\nP /= P.sum(dim=1, keepdim=True)\n\n\nP[0,:].sum(), P[:,0].sum()\n\n(tensor(1.), tensor(3.0023))\n\n\n\ng = torch.Generator().manual_seed(2147483647)\n\nfor _ in range(5):\n    out=[]\n    ix = 0\n    while True:\n        p = P[ix]\n        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        out.append(itos[ix])\n        if ix == 0:\n            break\n    print(''.join(out))\n\nmor.\naxx.\nminaymoryles.\nkondlaisah.\nanchshizarie.",
    "crumbs": [
      "Build makemore yay"
    ]
  },
  {
    "objectID": "build_makemore_yay.html#evaluate-model-quality",
    "href": "build_makemore_yay.html#evaluate-model-quality",
    "title": "Build makemore yay",
    "section": "Evaluate model quality",
    "text": "Evaluate model quality\nNegative log-likelihood measures the quality. The lower this is the better (0 being the best).\n\nlog_likelihood = 0.\nn = 0\nfor w in ['andrejq']:#words:\n    chs = ['.'] + list(w) + ['.']\n    for ch1,ch2 in zip(chs,chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        prob = P[ix1,ix2]\n        logprob = torch.log(prob)\n        log_likelihood += logprob\n        n += 1\n#         print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\nprint(f'{log_likelihood=:.4f}')        \nnll = -log_likelihood\nprint(f'{nll/n=:.4f}')\n\nlog_likelihood=-27.8672\nnll/n=3.4834",
    "crumbs": [
      "Build makemore yay"
    ]
  },
  {
    "objectID": "build_makemore_yay.html#training-set",
    "href": "build_makemore_yay.html#training-set",
    "title": "Build makemore yay",
    "section": "Training set",
    "text": "Training set\nCreate the training set of bigrams (x,y)\n\nxs,ys = [],[]\n\nfor w in words[:1]:\n    chs = ['.'] + list(w) + ['.']\n    for ch1,ch2 in zip(chs,chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        xs.append(ix1)\n        ys.append(ix2)\n\n# prefer to use torch.tensor instead of torch.Tensor\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\n\n\nxs, ys\n\n(tensor([ 0,  5, 13, 13,  1]), tensor([ 5, 13, 13,  1,  0]))\n\n\n\nimport torch.nn.functional as F\n\n\n# we want things going into the neural network \n# to be floats so we cast. Otherwise we would\n# end up with int64\nxenc = F.one_hot(xs, num_classes=27).float()\n\n\nxenc.shape\n\ntorch.Size([5, 27])\n\n\n\nxenc[:2]\n\ntensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n\n\n\nplt.imshow(xenc[:2])\n\n\n\n\n\n\n\n\n\nxenc.dtype\n\ntorch.float32\n\n\n\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27,27), generator=g) #27 neurons each getting 27 inputs\n\nnumber of examples: 5\n\n\n\nlogits = xenc@W #log-counts\ncounts = logits.exp() # exponentiate the logits to get fake counts\nprobs = counts/counts.sum(1,keepdims=True)\nprobs\n\ntensor([[0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n         0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n         0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n        [0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n         0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n         0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n        [0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n         0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n         0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091]])\n\n\n\nprobs[0,:].sum(), probs[:,0].sum(), probs.shape\n\n(tensor(1.0000), tensor(0.1670), torch.Size([5, 27]))\n\n\nForward Pass\n\n# create the datases\nxs,ys = [],[]\n\nfor w in words:\n    chs = ['.'] + list(w) + ['.']\n    for ch1,ch2 in zip(chs,chs[1:]):\n        ix1 = stoi[ch1]\n        ix2 = stoi[ch2]\n        xs.append(ix1)\n        ys.append(ix2)\n\n# prefer to use torch.tensor instead of torch.Tensor\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint(f'number of examples: {num}')\n\n# initialize the network  \n\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((27,27), generator=g, requires_grad=True) #single layer of 27 neurons each getting 27 inputs\n\nnumber of examples: 228146\n\n\n\nloss = None\n\n\nfor k in range(100):\n    xenc = F.one_hot(xs, num_classes = 27).float()\n    logits = xenc@W #log-counts\n    counts = logits.exp() # exponentiate the logits to get fake counts\n    probs = counts/counts.sum(1,keepdims=True)\n    loss = (-(probs[torch.arange(num),ys]).log()).mean()\n#     print(loss.item())\n    \n    #backward pass\n    W.grad = None #More efficient than setting to zero directly. Lack of gradient is interpreted as zero by PyTorch\n    loss.backward()\n    \n    #update\n    W.data += -50 * W.grad    \n\nprint(loss.item())\n\n2.462392807006836\n\n\n\n# with regularization\nfor k in range(100):\n    xenc = F.one_hot(xs, num_classes = 27).float()\n    logits = xenc@W #log-counts\n    counts = logits.exp() # exponentiate the logits to get fake counts\n    probs = counts/counts.sum(1,keepdims=True)\n    loss = (-(probs[torch.arange(num),ys]).log()).mean() + 0.01*(W**2).mean()\n#     print(loss.item())\n    \n    #backward pass\n    W.grad = None #More efficient than setting to zero directly. Lack of gradient is interpreted as zero by PyTorch\n    loss.backward()\n    \n    #update\n    W.data += -50 * W.grad    \n\nprint(loss.item())\n\n2.4829957485198975",
    "crumbs": [
      "Build makemore yay"
    ]
  },
  {
    "objectID": "makemore_bn.html",
    "href": "makemore_bn.html",
    "title": "Makemore Part 3",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\nwords = open('../names.txt','r').read().splitlines()\nwords[:8]\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\nlen(words)\n\n32033\n''.join(words[:2]), set(''.join(words[:2]))\n\n('emmaolivia', {'a', 'e', 'i', 'l', 'm', 'o', 'v'})\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0 #add special dot character to the vocabulary\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n# build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n    X, Y = [], []\n\n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix] # crop and append\n    \n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\nb1 = torch.randn(n_hidden,                        generator=g)\nW2 = torch.randn((n_hidden, vocab_size),          generator=g)\nb2 = torch.randn(vocab_size,                      generator=g)\n\nparameters = [C, W1, W2, b2]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n11697\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n    break\n\n      0/ 200000: 27.8817\nThe initial loss is way too high because the net is assigning a large probability to an incorrect prediction (this means that the correct next character has a teeny probability assigned to it and we end up getting hit with a high loss). Recall that log of values close to 0 goes to minus infinity.\n# initial logits all over the place. Resulting in misplaced high confidence\n# on an incorrect prediction\nlogits[0]\n\ntensor([ -2.3527,  36.4366, -10.7306,   5.7165,  18.6409, -11.6998,  -2.1991,\n          1.8535,  10.9996,  10.6730,  12.3507, -10.3809,   4.7243, -24.4257,\n         -8.5909,   1.9024, -12.2744, -12.4751, -23.2778,  -2.0163,  25.8767,\n         14.2108,  17.7691, -10.9204, -20.7335,   6.4560,  11.1615],\n       grad_fn=&lt;SelectBackward0&gt;)\n# However what we want the net to do is to output uniform probabilities \n# for the next character and hence obtain the following loss:\n-torch.tensor(1/27.).log()\n\ntensor(3.2958)\n# infinite loss for this contrived 4-d example\nlogits = torch.tensor([0.,5000.,0.,0.])\nprobs = torch.softmax(logits, dim=0)\nloss = -probs[2].log()\nlogits, probs, loss\n\n(tensor([   0., 5000.,    0.,    0.]), tensor([0., 1., 0., 0.]), tensor(inf))\n# rather we want at the initial step for the net\n# to output a uniform probability for the next\n# character\n# Though we only want them to be roughly the same\n# and not exactly the same for the purposes of \n# symmetry breaking.\nlogits = torch.tensor([0.,0.,0.,0.])\nprobs = torch.softmax(logits, dim=0)\nloss = -probs[2].log()\nlogits, probs, loss\n\n(tensor([0., 0., 0., 0.]),\n tensor([0.2500, 0.2500, 0.2500, 0.2500]),\n tensor(1.3863))\nSo in the output layer let’s set:\nso they cannot contribute to the initial predictions being all over the place.\n# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\nb1 = torch.randn(n_hidden,                        generator=g)\nW2 = torch.randn((n_hidden, vocab_size),          generator=g)*0.01\nb2 = torch.randn(vocab_size,                      generator=g)*0\n\nparameters = [C, W1, W2, b2]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n11697\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n    break\n\n      0/ 200000: 3.3221\nplt.figure(figsize=(20,10))\nplt.imshow(h.abs() &gt; 0.99, cmap='gray', interpolation='nearest')#white if True i.e., activations are  &gt; 0.99 (and Black if False)\nIn the plot above, each row is a training example and each column is one of our 200 neurons. We would be in trouble if any column was completely white. That means that we have a dead neuron and gradient is not flowing back.\nplt.hist(h.view(-1).tolist(),bins=50);\ntanh saturates, roughly speaking, if the inputs are &gt; 5 or &lt;= -5\nplt.hist(hpreact.view(-1).tolist(),bins=50);\nplt.plot(lossi)\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    emb = C[x] # (N, block_size, n_embd)\n    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n    hpreact = embcat @ W1 + b1\n    h = torch.tanh(hpreact) # (N, n_hidden)\n    logits = h @ W2 + b2 # (N, vocab_size)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 3.187894582748413\nval 3.1877975463867188\n# sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    out=[]\n    context = [0]*block_size\n    while True:\n        emb = C[torch.tensor([context])]\n        h = torch.tanh(emb.view((1,-1)) @ W1 + b1)\n        logits = h @ W2 + b2\n        probs = F.softmax(logits, dim=1)\n        \n        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n        context = context[1:]+[ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    print(''.join([itos[i] for i in out]))\n\ncbrpzhxqtb.\nhqvufkwmrpxqthtykhkcasskejrzhnbnfqplsyhc.\nvgqhiu.\nrrmuqptcmziivdyxlhggphhlm.\npoin.\nqbqjtzsrlivwa.\nvvdbquwqzthogdjaryxixfkqeupiusdbwed.\necoia.\ngtlffhysflquhpagmbvdjhksyjrpmqqosozswjcojqmwyljifrenqkpfsadlnuo.\nzoebsrkoiazhrynhr.\nopklhynrxyh.\nxqioloqbplbvbwdn.\nip.\nqmuitjpbvfpzpddgpycsislqwkkmco.\nzauqnyjydpk.\nkvweskatikzamdtevl.\nky.\nqdyltoorowooktbymouokfbfcw.\nzoindzcs.\nau.\nAt this point we understand that our hidden layer is sub-optimal from the learning perspective since a lot of the neurons are saturated (in the tails of the tanh). Let’s fix that\n# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.2 # make these smaller\nb1 = torch.randn(n_hidden,                        generator=g) * 0.01 #small entropy\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\nb2 = torch.randn(vocab_size,                      generator=g) * 0\n\nparameters = [C, W1, W2, b2]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n11697\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n    break\n\n      0/ 200000: 3.3135\nplt.figure(figsize=(20,10))\nplt.imshow(h.abs() &gt; 0.99, cmap='gray', interpolation='nearest')#white if True i.e., activations are  &gt; 0.99 (and Black if False)\nSo we see that no neurons are saturated.\nplt.hist(h.view(-1).tolist(),bins=50);\nplt.hist(hpreact.view(-1).tolist(),bins=50);\nNow let’s run the full optimization without a break\n# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.2 # make these smaller\nb1 = torch.randn(n_hidden,                        generator=g) * 0.01 #small entropy\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\nb2 = torch.randn(vocab_size,                      generator=g) * 0\n\nparameters = [C, W1, W2, b2]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n11697\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n#     break\n\n      0/ 200000: 3.3135\n  10000/ 200000: 2.1715\n  20000/ 200000: 2.3240\n  30000/ 200000: 2.4010\n  40000/ 200000: 1.9887\n  50000/ 200000: 2.3029\n  60000/ 200000: 2.4564\n  70000/ 200000: 2.1249\n  80000/ 200000: 2.3069\n  90000/ 200000: 2.1057\n 100000/ 200000: 1.8948\n 110000/ 200000: 2.2326\n 120000/ 200000: 1.9659\n 130000/ 200000: 2.4410\n 140000/ 200000: 2.2106\n 150000/ 200000: 2.1795\n 160000/ 200000: 1.8383\n 170000/ 200000: 1.8131\n 180000/ 200000: 1.9572\n 190000/ 200000: 1.8002\nplt.plot(lossi)\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    emb = C[x] # (N, block_size, n_embd)\n    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n    hpreact = embcat @ W1 + b1\n    h = torch.tanh(hpreact) # (N, n_hidden)\n    logits = h @ W2 + b2 # (N, vocab_size)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 2.036555767059326\nval 2.103057622909546",
    "crumbs": [
      "Makemore Part 3"
    ]
  },
  {
    "objectID": "makemore_bn.html#principled-ways-to-set-the-scaling-parameters",
    "href": "makemore_bn.html#principled-ways-to-set-the-scaling-parameters",
    "title": "Makemore Part 3",
    "section": "Principled ways to set the scaling parameters",
    "text": "Principled ways to set the scaling parameters\nHow to come up with the numbers that multiply the different weight matrices and the biases?\n\nx = torch.randn(1000,10)\ngain = 1\nw = torch.randn(10,200) * (gain / (10**0.5)) #scale by square root of the fan-in(the number of inputs)\ny = (x@w)\nprint(x.mean(),x.std())\nprint(y.mean(),y.std())\nplt.figure(figsize=(20,5))\nplt.subplot(121)\nplt.hist(x.view(-1).tolist(),50,density=True);\nplt.subplot(122)\nplt.hist(y.view(-1).tolist(),50,density=True);\n\ntensor(-0.0022) tensor(1.0005)\ntensor(-0.0017) tensor(1.0200)\n\n\n\n\n\n\n\n\n\nKaiming He’s paper Delving Deep into rectifiers..\nThe big picture is that we want the activations to be unit normal. The problem for tanh is if the activations going into them are too small then the tanh is inactive and if they are too big then the tanh saturates and then nothing flows back in the backward pass (since t = 1)).\nAlso look at torch.nn.init and then we will see Kaiming normal.\nWe need a gain term whenever we have a contractive transformation such as relu (which throws away half of the incoming inputs and sets them to zero) or tanh and so on. So to fight this “squeezing in” we have to boost the weights so we can get outputs that are standard Gaussian.\nThe use of things like residual connections, normalization layers (batch, layer etc), better optimizers (RMSProp and ADAM) have made training of deep networks much less finicky but circa 2016 you would have to be extremely careful about setting these properly.\nWhat does Andrej do? He normalizes the weights by the square root of the fan-in.\n\ntest = torch.randn(1000000)\ntest.mean(), test.std()\n\n(tensor(0.0020), tensor(0.9987))\n\n\nRecall that multiplying the Normal distribution by a number changes it’s standard deviation.\n\n(test*2).mean(), (test*2).std()\n\n(tensor(0.0040), tensor(1.9975))\n\n\n\nhpreact.mean(dim=0,keepdim=True).shape\n\ntorch.Size([1, 200])\n\n\n\nhpreact.std(dim=0,keepdim=True).shape\n\ntorch.Size([1, 200])\n\n\n\n# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * ((5/3)/(n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.01 #small entropy\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * ((1)/(n_hidden)**0.5)\nb2 = torch.randn(vocab_size,                      generator=g) * 0\n\nparameters = [C, W1, W2, b2]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n11697\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n    \n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n#     break\n\n      0/ 200000: 3.6625\n  10000/ 200000: 2.2262\n  20000/ 200000: 2.3770\n  30000/ 200000: 2.5023\n  40000/ 200000: 1.9618\n  50000/ 200000: 2.3722\n  60000/ 200000: 2.5106\n  70000/ 200000: 2.1565\n  80000/ 200000: 2.2779\n  90000/ 200000: 2.1759\n 100000/ 200000: 1.8739\n 110000/ 200000: 2.0963\n 120000/ 200000: 1.9099\n 130000/ 200000: 2.3631\n 140000/ 200000: 2.0883\n 150000/ 200000: 2.1785\n 160000/ 200000: 1.8354\n 170000/ 200000: 1.7960\n 180000/ 200000: 1.9382\n 190000/ 200000: 1.8669\n\n\n\nplt.plot(lossi)\n\n\n\n\n\n\n\n\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    emb = C[x] # (N, block_size, n_embd)\n    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n    hpreact = embcat @ W1 + b1\n    h = torch.tanh(hpreact) # (N, n_hidden)\n    logits = h @ W2 + b2 # (N, vocab_size)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 2.0383036136627197\nval 2.1061367988586426",
    "crumbs": [
      "Makemore Part 3"
    ]
  },
  {
    "objectID": "makemore_bn.html#batch-normalization",
    "href": "makemore_bn.html#batch-normalization",
    "title": "Makemore Part 3",
    "section": "Batch Normalization",
    "text": "Batch Normalization\n\n# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * ((5/3)/(n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.01 #small entropy\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * ((1)/(n_hidden)**0.5)\nb2 = torch.randn(vocab_size,                      generator=g) * 0\n\nbngain = torch.ones((1,n_hidden))\nbnbias = torch.zeros((1,n_hidden))\n\n\nparameters = [C, W1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n12097\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n    hpreact = bngain*(hpreact - hpreact.mean(dim=0,keepdim=True))/hpreact.std(dim=0,keepdim=True) + bnbias\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n#     break\n\n      0/ 200000: 3.5772\n  10000/ 200000: 2.1652\n  20000/ 200000: 2.4167\n  30000/ 200000: 2.4436\n  40000/ 200000: 2.0111\n  50000/ 200000: 2.3681\n  60000/ 200000: 2.4293\n  70000/ 200000: 2.1056\n  80000/ 200000: 2.3539\n  90000/ 200000: 2.1389\n 100000/ 200000: 1.9417\n 110000/ 200000: 2.3648\n 120000/ 200000: 1.9323\n 130000/ 200000: 2.4825\n 140000/ 200000: 2.3174\n 150000/ 200000: 2.1054\n 160000/ 200000: 1.9776\n 170000/ 200000: 1.8040\n 180000/ 200000: 1.9797\n 190000/ 200000: 1.8384\n\n\n\nplt.plot(lossi)\n\n\n\n\n\n\n\n\n\n# calibrate the batch norm at the end of training\n\nwith torch.no_grad():\n    # pass the training set through\n    emb = C[Xtr]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ W1 + b1\n    # measure the mean/std over the entire training set \n    bnmean = hpreact.mean(dim=0,keepdim=True)\n    bnstd = hpreact.std(dim=0,keepdim=True)\n\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    emb = C[x] # (N, block_size, n_embd)\n    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n    hpreact = embcat @ W1 + b1\n    hpreact = bngain*(hpreact - bnmean)/bnstd + bnbias    \n    h = torch.tanh(hpreact) # (N, n_hidden)\n    logits = h @ W2 + b2 # (N, vocab_size)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 2.068721294403076\nval 2.1081509590148926",
    "crumbs": [
      "Makemore Part 3"
    ]
  },
  {
    "objectID": "makemore_bn.html#keep-running-track-of-bnmean-and-bnstd",
    "href": "makemore_bn.html#keep-running-track-of-bnmean-and-bnstd",
    "title": "Makemore Part 3",
    "section": "Keep running track of bnmean and bnstd",
    "text": "Keep running track of bnmean and bnstd\nNobody wants to have a separate step at the end of training where we calculate these stats. So we just calculate them during training.\n\n# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * ((5/3)/(n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.01 #small entropy\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * ((1)/(n_hidden)**0.5)\nb2 = torch.randn(vocab_size,                      generator=g) * 0\n\nbngain = torch.ones((1,n_hidden))\nbnbias = torch.zeros((1,n_hidden))\n\nbnmean_running = torch.zeros((1,n_hidden))\nbnstd_running = torch.ones((1,n_hidden))\n\nparameters = [C, W1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n12097\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    hpreact = embcat @ W1 + b1 # hidden layer pre-activation\n    bnmeani = hpreact.mean(dim=0,keepdim=True)\n    bnstdi = hpreact.std(dim=0,keepdim=True)\n    hpreact = bngain*(hpreact - bnmeani)/bnstdi + bnbias\n    \n    with torch.no_grad():\n        bnmean_running = 0.999*bnmean_running + 0.001*bnmeani\n        bnstd_running = 0.999*bnstd_running + 0.001*bnstdi\n    \n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n\n      0/ 200000: 3.5772\n  10000/ 200000: 2.1652\n  20000/ 200000: 2.4167\n  30000/ 200000: 2.4436\n  40000/ 200000: 2.0111\n  50000/ 200000: 2.3681\n  60000/ 200000: 2.4293\n  70000/ 200000: 2.1056\n  80000/ 200000: 2.3539\n  90000/ 200000: 2.1389\n 100000/ 200000: 1.9417\n 110000/ 200000: 2.3648\n 120000/ 200000: 1.9323\n 130000/ 200000: 2.4825\n 140000/ 200000: 2.3174\n 150000/ 200000: 2.1054\n 160000/ 200000: 1.9776\n 170000/ 200000: 1.8040\n 180000/ 200000: 1.9797\n 190000/ 200000: 1.8384\n\n\n\nplt.plot(lossi)\n\n\n\n\n\n\n\n\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    emb = C[x] # (N, block_size, n_embd)\n    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n    hpreact = embcat @ W1 + b1\n    hpreact = bngain*(hpreact - bnmean_running)/bnstd_running + bnbias  \n    h = torch.tanh(hpreact) # (N, n_hidden)\n    logits = h @ W2 + b2 # (N, vocab_size)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 2.068506956100464\nval 2.108386993408203\n\n\nAnother point is that we are being wasteful here since b1 plays no role. The batch normalization step cancels out it’s effect. So we should just remove it and let the bnbias take over it’s place.\n\n# MLP revisited\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * ((5/3)/(n_embd * block_size)**0.5)\n# b1 = torch.randn(n_hidden,                        generator=g) * 0.01 #small entropy\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * ((1)/(n_hidden)**0.5)\nb2 = torch.randn(vocab_size,                      generator=g) * 0\n\nbngain = torch.ones((1,n_hidden))\nbnbias = torch.zeros((1,n_hidden))\n\nbnmean_running = torch.zeros((1,n_hidden))\nbnstd_running = torch.ones((1,n_hidden))\n\nparameters = [C, W1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n12097\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\nmomentum = 0.001\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n    \n    #Batch Norm Layer\n    bnmeani = hpreact.mean(dim=0,keepdim=True)\n    bnstdi = hpreact.std(dim=0,keepdim=True)\n    hpreact = bngain*(hpreact - bnmeani)/bnstdi + bnbias #should add a small epsilon to prevent divide by zero\n    \n    with torch.no_grad():\n        bnmean_running = (1-momentum)*bnmean_running + momentum*bnmeani\n        bnstd_running = (1-momentum)*bnstd_running + momentum*bnstdi\n    # Batch Norm\n    # Nonlinearity\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n\n      0/ 200000: 3.6702\n  10000/ 200000: 2.0310\n  20000/ 200000: 2.5991\n  30000/ 200000: 2.0311\n  40000/ 200000: 2.2747\n  50000/ 200000: 1.8810\n  60000/ 200000: 2.1123\n  70000/ 200000: 2.3104\n  80000/ 200000: 2.3838\n  90000/ 200000: 2.0555\n 100000/ 200000: 2.3456\n 110000/ 200000: 2.2557\n 120000/ 200000: 1.6611\n 130000/ 200000: 1.8733\n 140000/ 200000: 2.3256\n 150000/ 200000: 1.9549\n 160000/ 200000: 2.0183\n 170000/ 200000: 2.3910\n 180000/ 200000: 2.1503\n 190000/ 200000: 2.0741\n\n\n\nplt.plot(lossi)\n\n\n\n\n\n\n\n\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    emb = C[x] # (N, block_size, n_embd)\n    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n    hpreact = embcat @ W1 + b1\n    hpreact = bngain*(hpreact - bnmean_running)/bnstd_running + bnbias  \n    h = torch.tanh(hpreact) # (N, n_hidden)\n    logits = h @ W2 + b2 # (N, vocab_size)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 2.068607807159424\nval 2.108011245727539\n\n\nhttps://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L143\nBottleneck blocks (repeating motif of conv, bn then relu) are repeated in series in a resnet.\nA good thing about batch normalization is that it provides some regularization but a con is that now all the input examples in a batch are coupled. More recently people are using group/layer/instance normalization.\n\n##### Let's train a deeper network\n# The classes we create here are the same API as nn.Module in PyTorch\n\nclass Linear:\n  \n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n        self.bias = torch.zeros(fan_out) if bias else None\n  \n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n  \n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n\n\nclass BatchNorm1d:\n  \n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running 'momentum update')\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n  \n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True) # batch mean\n            xvar = x.var(0, keepdim=True) # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        return self.out\n  \n    def parameters(self):\n        return [self.gamma, self.beta]\n\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n    def parameters(self):\n        return []\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 100 # the number of neurons in the hidden layer of the MLP\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\n\nC = torch.randn((vocab_size, n_embd),            generator=g)\n\nlayers = [\n  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n]\n\n# layers = [\n#   Linear(n_embd * block_size, n_hidden), Tanh(),\n#   Linear(           n_hidden, n_hidden), Tanh(),\n#   Linear(           n_hidden, n_hidden), Tanh(),\n#   Linear(           n_hidden, n_hidden), Tanh(),\n#   Linear(           n_hidden, n_hidden), Tanh(),\n#   Linear(           n_hidden, vocab_size),\n# ]\n\n# layers = [\n#   Linear(n_embd * block_size, n_hidden), #Tanh(),\n#   Linear(           n_hidden, n_hidden), #Tanh(),\n#   Linear(           n_hidden, n_hidden), #Tanh(),\n#   Linear(           n_hidden, n_hidden), #Tanh(),\n#   Linear(           n_hidden, n_hidden), #Tanh(),\n#   Linear(           n_hidden, vocab_size),\n# ]\n\nwith torch.no_grad():\n    # last layer: make less confident\n    layers[-1].gamma *= 0.1\n    #layers[-1].weight *= 0.1\n    # all other layers: apply gain\n    for layer in layers[:-1]:\n        if isinstance(layer, Linear):\n            layer.weight *= 5/3 #Try 10 or 0.5 to see what happens instead of 5/3\n\nparameters = [C] + [p for layer in layers for p in layer.parameters()]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n47024\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\nud = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n  \n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, Yb) # loss function\n  \n    # backward pass\n    for layer in layers:\n        layer.out.retain_grad() # AFTER_DEBUG: would need to take out retain_grad\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n  \n    # update\n    lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n    # Keep track of the update to data ratio\n    with torch.no_grad():\n        ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n\n    if i &gt; 1000:\n        break # AFTER_DEBUG: would take out obviously to run full optimization\n\n      0/ 200000: 3.2870\n\n\n\n# visualize histograms\nplt.figure(figsize=(20, 4)) # width and height of the plot\nlegends = []\nfor i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n    if isinstance(layer, Tanh):\n        t = layer.out\n        print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() &gt; 0.97).float().mean()*100))\n        hy, hx = torch.histogram(t, density=True)\n        plt.plot(hx[:-1].detach(), hy.detach())\n        legends.append(f'layer {i} ({layer.__class__.__name__})')\nplt.legend(legends);\nplt.title('activation distribution')\n\nlayer 2 (      Tanh): mean -0.00, std 0.63, saturated: 2.97%\nlayer 5 (      Tanh): mean +0.01, std 0.64, saturated: 2.41%\nlayer 8 (      Tanh): mean -0.00, std 0.64, saturated: 2.44%\nlayer 11 (      Tanh): mean +0.00, std 0.64, saturated: 2.22%\nlayer 14 (      Tanh): mean -0.00, std 0.64, saturated: 2.25%\n\n\nText(0.5, 1.0, 'activation distribution')\n\n\n\n\n\n\n\n\n\n\n# visualize histograms\n# We want the gradients for all of the layers to be roughly the same\n# none of them shrinking to zero or exploding\nplt.figure(figsize=(20, 4)) # width and height of the plot\nlegends = []\nfor i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n    if isinstance(layer, Tanh):\n        t = layer.out.grad\n        print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n        hy, hx = torch.histogram(t, density=True)\n        plt.plot(hx[:-1].detach(), hy.detach())\n        legends.append(f'layer {i} ({layer.__class__.__name__})')\nplt.legend(legends);\nplt.title('gradient distribution')\n\nlayer 2 (      Tanh): mean +0.000000, std 3.561966e-03\nlayer 5 (      Tanh): mean +0.000000, std 3.119518e-03\nlayer 8 (      Tanh): mean -0.000000, std 2.904400e-03\nlayer 11 (      Tanh): mean +0.000000, std 2.580221e-03\nlayer 14 (      Tanh): mean +0.000000, std 2.440870e-03\n\n\nText(0.5, 1.0, 'gradient distribution')\n\n\n\n\n\n\n\n\n\n\n# visualize histograms\nplt.figure(figsize=(20, 4)) # width and height of the plot\nlegends = []\nfor i,p in enumerate(parameters):\n    t = p.grad\n    if p.ndim == 2: #skip biases and gammas and betas of the batch norm\n        print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n        hy, hx = torch.histogram(t, density=True)\n        plt.plot(hx[:-1].detach(), hy.detach())\n        legends.append(f'{i} {tuple(p.shape)}')\nplt.legend(legends)\nplt.title('weights gradient distribution');\n\nweight   (27, 10) | mean +0.000000 | std 8.198624e-03 | grad:data ratio 8.190266e-03\nweight  (30, 100) | mean -0.000195 | std 7.650370e-03 | grad:data ratio 2.447689e-02\nweight (100, 100) | mean -0.000097 | std 6.207553e-03 | grad:data ratio 3.712865e-02\nweight (100, 100) | mean +0.000015 | std 5.561911e-03 | grad:data ratio 3.297624e-02\nweight (100, 100) | mean +0.000008 | std 5.220592e-03 | grad:data ratio 3.110145e-02\nweight (100, 100) | mean +0.000029 | std 4.829152e-03 | grad:data ratio 2.876363e-02\nweight  (100, 27) | mean -0.000014 | std 9.281198e-03 | grad:data ratio 5.568807e-02\n\n\n\n\n\n\n\n\n\n\n# We want all the parameters to be training at roughly the same rate\nplt.figure(figsize=(20, 4))\nlegends = []\nfor i,p in enumerate(parameters):\n    if p.ndim == 2:\n        plt.plot([ud[j][i] for j in range(len(ud))])\n        legends.append('param %d' % i)\nplt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\nplt.legend(legends);\n\n\n\n\n\n\n\n\nFin: 1h55m57s/1h55m557s",
    "crumbs": [
      "Makemore Part 3"
    ]
  },
  {
    "objectID": "l01_e01.html",
    "href": "l01_e01.html",
    "title": "L01_E01",
    "section": "",
    "text": "Train a trigram language model, i.e. take two characters as an input to predict the 3rd one. Feel free to use either counting or a neural net. Evaluate the loss; Did it improve over a bigram model?\nwords = open('../names.txt', 'r').read().splitlines()\nlen(words)\n\n32033\nwords[:5]\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia']\nThe model will predict the next character given the two preceding characters.\n# Each entry is a training example\nfor word in words[:2]:\n    chs = '..'+word+'.'\n    for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n        print(ch1,ch2,ch3)\n\n. . e\n. e m\ne m m\nm m a\nm a .\n. . o\n. o l\no l i\nl i v\ni v i\nv i a\ni a .\nchars = sorted(list(set(''.join(words))))\nlen(chars)\n\n26\nAdd the character that marks the beginning of string. This character will be used twice for when we want to indicate that we are at the beginning of a potential name.\nctoi = {c : i+1 for i,c in enumerate(chars)}\nctoi['.']=0\nitoc = {i:c for c,i in ctoi.items()}\nnum_chars = len(ctoi.keys())\nnum_chars\n\n27\nCreate a dictionary that takes two characters and returns an integer. This will be used when sampling from the model.\nstoi = {}\nfor i0,c0 in sorted(itoc.items(), key=lambda kv: kv[0]):\n    for i1,c1 in sorted(itoc.items(), key=lambda kv: kv[0]):\n        #print((i0*num_chars) + i1,c0,c1)\n        stoi[(c0,c1)] = (i0*num_chars) + i1\nstoi[('.','.')], stoi[('z','z')]\n\n(0, 728)",
    "crumbs": [
      "L01_E01"
    ]
  },
  {
    "objectID": "l01_e01.html#sampling-method-1",
    "href": "l01_e01.html#sampling-method-1",
    "title": "L01_E01",
    "section": "Sampling Method 1",
    "text": "Sampling Method 1\n\ng = torch.Generator().manual_seed(2147483647)\nfor _ in range(10):\n    out=[]\n    ix = 0 #start off with ..\n    while True:\n        p = P[ix]\n        next_char_idx = torch.multinomial(p, \n                                          num_samples=1, \n                                          replacement=True, \n                                          generator=g\n                                         ).item()\n        next_char = itoc[next_char_idx]\n        \n        #tack on the next character\n        out.append(next_char)\n        \n        if next_char_idx == 0:\n            break\n\n        # if only a single entry in out this means we are just starting out\n        # so we add a dot to the first character sampled from the trigram\n        # model\n        ix = stoi[('.',out[-1])] if len(out)==1 else stoi[(out[-2],out[-1])]\n    \n    print(''.join(out))\n\nce.\nbra.\njalius.\nrochityharlonimittain.\nluwak.\nka.\nda.\nsamiyah.\njaver.\ngotai.\n\n\nAnother way to sample where we fill the out with the desired starting characters.",
    "crumbs": [
      "L01_E01"
    ]
  },
  {
    "objectID": "l01_e01.html#sampling-method-2",
    "href": "l01_e01.html#sampling-method-2",
    "title": "L01_E01",
    "section": "Sampling Method 2",
    "text": "Sampling Method 2\n\ng = torch.Generator().manual_seed(2147483647)\nfor _ in range(20):\n    out=['.','.']\n    ix = stoi[(out[-2],out[-1])]\n    while True:\n        p = P[ix]\n        next_char_idx = torch.multinomial(p, \n                                          num_samples=1, \n                                          replacement=True, \n                                          generator=g\n                                         ).item()\n        next_char = itoc[next_char_idx]\n        \n        #tack on the next character\n        out.append(next_char)\n        \n        if next_char_idx == 0:\n            break\n        \n        ix = stoi[(out[-2],out[-1])]\n    \n    print(''.join(out).replace('.',''))\n\nce\nbra\njalius\nrochityharlonimittain\nluwak\nka\nda\nsamiyah\njaver\ngotai\nmoriellavojkwuthda\nkaley\nmaside\nen\naviyah\nfobspihiliven\ntahlasuzusfxx\nan\nglhpynn\nisan",
    "crumbs": [
      "L01_E01"
    ]
  },
  {
    "objectID": "l01_e01.html#sampling-uniformly-at-random",
    "href": "l01_e01.html#sampling-uniformly-at-random",
    "title": "L01_E01",
    "section": "Sampling Uniformly at Random",
    "text": "Sampling Uniformly at Random\nThe Trigram model does not seem like it is working i.e, the output does not seem very namelike. So let’s replace sampling from P with sampling for the next character from a uniform distributions and see the results.\nNow we see that the results are essentially garbage for the untrained model that is just picking the next character uniformly at random.\n\ng = torch.Generator().manual_seed(2147483647)\nfor _ in range(50):\n    out=[]\n    ix = 0 #start off with ..\n    while True:\n        p = torch.ones(num_chars)/(1.0*num_chars)\n        next_char_idx = torch.multinomial(p, \n                                          num_samples=1, \n                                          replacement=True, \n                                          generator=g\n                                         ).item()\n        next_char = itoc[next_char_idx]\n        \n        #tack on the next character\n        out.append(next_char)\n        \n        if next_char_idx == 0:\n            break\n\n        # if only a single entry in out this means we are just starting out\n        # so we add a dot to the first character sampled from the trigram\n        # model\n        ix = stoi[('.',out[-1])] if len(out)==1 else stoi[(out[-2],out[-1])]\n    \n    print(''.join(out))\n\ncexzm.\nzoglkurkicqzktyhwmvmzimjttainrlkfukzkktda.\nsfcxvpubjtbhrmgotzx.\niczixqctvujkwptedogkkjemkmmsidguenkbvgynywftbspmhwcivgbvtahlvsu.\ndsdxxblnwglhpyiw.\nigwnjwrpfdwipkwzkm.\ndesu.\nfirmt.\ngbiksjbquabsvoth.\nkuysxqevhcmrbxmcwyhrrjenvxmvpfkmwmghfvjzxobomysox.\ngbptjapxweegpfwhccfyzfvksiiqmvwbhmiwqmdgzqsamjhgamcxwmmk.\niswcxfmbalcslhy.\nfpycvasvz.\nbqzazeunschck.\nwnkojuoxyvtvfiwksddugnkul.\nfuwfcgjz.\nabl.\nj.\nnuuutstofgqzubbo.\nrdubpknhmd.\nvhfacdvaaasjzjkdh.\ngh.\nfrdhlhahflrklmlcugro.\npnxhayx.\nvn.\ngixgosfqn.\nmempfnclfxtirbqhvjfdwhzymayerzqvmzjvtjuifbooocnkcxjkvsmjafciekxoraw.\n.\nveigtbcaamnef.\nchfeukwowgsadjjkkswrcpawhoxskfikwbscynndmiuxxwoturzhqnsjdndsziocnbxiegzzulhnqdqwosi.\nkdwnfjvmtthtpzbmdvvrvtptaqlhdnkj.\nzxkcbczsrcagitwicvkcqiotgnvpllciqs.\nuohjxnvxqikebadkdawdfwwha.\nfqcnmrpoljlpjldyjehpprjppsmkzdhrmgyoadmsod.\ndnvzcobtzfikidecxjhbmmjxqphvtedjbwkxzhisndnoauiycrdfetifkvzlzf.\nud.\nckndsgyldqbkcylrozgwkjgftrahdrnfapspdayna.\nthavpgelvlfqxxsdabgxpyzv.\nikzvrykvyxhuj.\nqkpwzuaics.\nxxqubplmqguhbpnetz.\n.\ntfscppboipyvqyrccgloodaengwalywfviiutynlldwbrpgklfmblqgkdhmoixqzls.\nq.\nsdjrgmtgxupikxluyhxreauxxjus.\nkfsngohzemqzdxusvoagrdvrpkzichkezabvtmclxzdftjsdpzpaxppovqambwhzg.\nfjrwrbhoyopsndmgny.\nahebb.\neprb.\ntyigehvatuoadruayixkhulmuthh.",
    "crumbs": [
      "L01_E01"
    ]
  },
  {
    "objectID": "l01_e01.html#evaluate-the-loss",
    "href": "l01_e01.html#evaluate-the-loss",
    "title": "L01_E01",
    "section": "Evaluate the loss",
    "text": "Evaluate the loss\n\nlog_likelihood = 0.\nn = 0\nfor word in words:\n    chs = '..' + word + '.'\n    for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n        #print(ch1,ch2,ch3)\n        prob = P[stoi[ch1,ch2],ctoi[ch3]]\n        logprob = torch.log(prob)\n        log_likelihood += logprob\n        n += 1\n\nprint(f'{log_likelihood=:.4f}') \nprint(f'{n=}')\nnll = -log_likelihood\nprint(f'{nll/n=:.4f}')\n\nlog_likelihood=-504653.0000\nn=228146\nnll/n=2.2120\n\n\nThe average negative log likelihood over the entire training set of the Bigram counting model was 2.4509 while that over the training set of the Trigram counting model is 2.21. Thus, we have an improvement.",
    "crumbs": [
      "L01_E01"
    ]
  },
  {
    "objectID": "l01_e01.html#training-set",
    "href": "l01_e01.html#training-set",
    "title": "L01_E01",
    "section": "Training Set",
    "text": "Training Set\n\nimport torch.nn.functional as F\n\n\nxs,ys = [],[]\n\nfor word in words:\n    chs = '..' + word + '.'\n    for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n        ix1 = stoi[ch1,ch2]\n        ix2 = ctoi[ch3]\n        xs.append(ix1)\n        ys.append(ix2)\n\n# prefer to use torch.tensor instead of torch.Tensor\nxs = torch.tensor(xs)\nys = torch.tensor(ys)\nnum = xs.nelement()\nprint(f'number of examples: {num}')\n\n# initialize the network  \n\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((num_chars*num_chars,num_chars), generator=g, requires_grad=True) #single layer of 27 neurons each getting 27x27 inputs\n\nnumber of examples: 228146\n\n\n\nfor k in range(400):\n    xenc = F.one_hot(xs, num_classes = num_chars*num_chars).float()\n    logits = xenc@W #log-counts\n    counts = logits.exp() # exponentiate the logits to get fake counts\n    probs = counts/counts.sum(1,keepdims=True)\n    loss = (-(probs[torch.arange(num),ys]).log()).mean()\n\n    if k%40==0:\n        print(loss.item())\n    \n    #backward pass\n    W.grad = None #More efficient than setting to zero directly. Lack of gradient is interpreted as zero by PyTorch\n    loss.backward()\n    \n    #update\n    W.data += -4*50 * W.grad    \n\nprint(loss.item())\n\n3.792776346206665\n2.44636607170105\n2.3429062366485596\n2.3829033374786377\n2.2747690677642822\n2.318384885787964\n2.3451452255249023\n2.244135856628418\n2.263615131378174\n2.2489423751831055\n2.315772771835327",
    "crumbs": [
      "L01_E01"
    ]
  },
  {
    "objectID": "l01_e01.html#sample-from-the-neural-network",
    "href": "l01_e01.html#sample-from-the-neural-network",
    "title": "L01_E01",
    "section": "Sample from the neural network",
    "text": "Sample from the neural network\n\ng = torch.Generator().manual_seed(2147483647)\n\nfor _ in range(5):\n    out=[]\n    ix = 0\n    while True:\n        xenc = F.one_hot(torch.tensor([ix]), num_classes = num_chars*num_chars).float()\n        logits = xenc@W #log-counts\n        counts = logits.exp() # exponentiate the logits to get fake counts\n        p = counts/counts.sum(1,keepdims=True)\n        #print(p)\n        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n        out.append(itoc[ix])\n        if ix == 0:\n            break\n        ix = stoi[('.',out[-1])] if len(out)==1 else stoi[(out[-2],out[-1])]\n    print(''.join(out))\n\nce.\nbra.\nemon.\nraila.\nkaydemmilistona.",
    "crumbs": [
      "L01_E01"
    ]
  },
  {
    "objectID": "l01_e01.html#evaluate-the-loss-1",
    "href": "l01_e01.html#evaluate-the-loss-1",
    "title": "L01_E01",
    "section": "Evaluate the loss",
    "text": "Evaluate the loss\n\nlog_likelihood = 0.\nn = 0\nfor word in words:\n    chs = '..' + word + '.'\n    for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n        ix = stoi[ch1,ch2]\n        xenc = F.one_hot(torch.tensor([ix]), num_classes = num_chars*num_chars).float()\n        logits = xenc@W #log-counts\n        counts = logits.exp() # exponentiate the logits to get fake counts\n        p = counts/counts.sum(1,keepdims=True)\n        #print(p.shape)\n        #print(ch1,ch2,ch3)\n        prob = p[0,ctoi[ch3]]\n        logprob = torch.log(prob)\n        log_likelihood += logprob\n        n += 1\n\nprint(f'{log_likelihood=:.4f}') \nprint(f'{n=}')\nnll = -log_likelihood\nprint(f'{nll/n=:.4f}')\n\nlog_likelihood=-511720.9688\nn=228146\nnll/n=2.2430",
    "crumbs": [
      "L01_E01"
    ]
  },
  {
    "objectID": "l01_e03.html",
    "href": "l01_e03.html",
    "title": "L01_E03",
    "section": "",
    "text": "Use the dev set to tune the strength of smoothing (or regularization) for the trigram model - i.e. try many possibilities and see which one works best based on the dev set loss. What patterns can you see in the train and dev set loss as you tune this strength? Take the best setting of the smoothing and evaluate on the test set once and at the end. How good of a loss do you achieve?\n\nimport torch\n\n\nimport random\nrandom.seed(42)\n\nwords = open('../names.txt','r').read().splitlines()\n\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nwords_tr = words[:n1]\nwords_dev = words[n1:n2]\nwords_te = words[n2:]\n\n\nchars = sorted(list(set(''.join(words_tr))))\nlen(chars)\n\n26\n\n\n\nctoi = {c : i+1 for i,c in enumerate(chars)}\nctoi['.'] = 0\n\n\nitoc = {i:c for c,i in ctoi.items()}\n\n\nnum_chars = len(ctoi.keys())\nnum_chars\n\n27\n\n\n\nstoi = {}\nfor i0,c0 in sorted(itoc.items(), key=lambda kv: kv[0]):\n    for i1,c1 in sorted(itoc.items(), key=lambda kv: kv[0]):\n        #print((i0*num_chars) + i1,c0,c1)\n        stoi[(c0,c1)] = (i0*num_chars) + i1\n\n\ndef build_dataset(words):\n    xs,ys = [],[]\n    \n    for word in words:\n        chs = '..' + word + '.'\n        for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n            ix1 = stoi[ch1,ch2]\n            ix2 = ctoi[ch3]\n            xs.append(ix1)\n            ys.append(ix2)\n    \n    # prefer to use torch.tensor instead of torch.Tensor\n    xs = torch.tensor(xs)\n    ys = torch.tensor(ys)\n    num = xs.nelement()\n    print(f'number of examples: {num}')   \n\n    return xs, ys\n\n\nXtr,Ytr=build_dataset(words_tr)\nXdev,Ydev=build_dataset(words_dev)\nXte,Yte=build_dataset(words_te)\n\nnumber of examples: 182625\nnumber of examples: 22655\nnumber of examples: 22866\n\n\n\nimport torch.nn.functional as F\n\n\nrege = torch.linspace(-4,1,200)\nregs = 10**rege\n\nloss_tr = []\nloss_dev = []\nloss_reg = [] #keep track of the loss we are adding from the regularization\nfor reg in regs:\n    g = torch.Generator().manual_seed(2147483647)\n    W = torch.randn((num_chars*num_chars,num_chars), generator=g, requires_grad=True) #single layer of 27 neurons each getting 27x27 inputs\n    for k in range(400):\n        xs, ys = Xtr, Ytr\n        xenc = F.one_hot(xs, num_classes = num_chars*num_chars).float()\n        logits = xenc@W #log-counts\n        counts = logits.exp() # exponentiate the logits to get fake counts\n        probs = counts/counts.sum(1,keepdims=True)\n        \n        loss_1 = (-(probs[torch.arange(xs.nelement()),ys]).log()).mean()\n        loss_2 = reg*(W**2).mean() #regularization loss\n        \n        loss = loss_1 + loss_2\n    \n        # if k%40==0: print(loss.item())\n        \n        #backward pass\n        W.grad = None #More efficient than setting to zero directly. Lack of gradient is interpreted as zero by PyTorch\n        loss.backward()\n        \n        #update\n        W.data += -4*50 * W.grad    \n\n    loss_tr.append(loss_1.item())\n    loss_reg.append(loss_2.item())\n    \n    xs, ys = Xdev, Ydev\n    xenc = F.one_hot(xs, num_classes = num_chars*num_chars).float()\n    logits = xenc@W #log-counts\n    counts = logits.exp() # exponentiate the logits to get fake counts\n    probs = counts/counts.sum(1,keepdims=True)\n    loss = (-(probs[torch.arange(xs.nelement()),ys]).log()).mean()\n    loss_dev.append(loss.item())\n\n    #print(reg.item(), loss_tr[-1], loss_dev[-1])\nprint('Done!')\n\nDone!\n\n\n\nimport pandas as pd\nimport numpy as np\n\n\ndf = pd.DataFrame({'rege': rege, 'reg': regs, 'loss_tr':loss_tr, 'loss_dev': loss_dev })\ndf.head()\n\n\n\n\n\n\n\n\nrege\nreg\nloss_tr\nloss_dev\n\n\n\n\n0\n-4.000000\n0.000100\n2.283918\n2.248096\n\n\n1\n-3.974874\n0.000106\n2.224400\n2.251100\n\n\n2\n-3.949749\n0.000112\n2.335932\n2.280850\n\n\n3\n-3.924623\n0.000119\n2.228556\n2.264807\n\n\n4\n-3.899498\n0.000126\n2.297669\n2.255722\n\n\n\n\n\n\n\n\n(df.set_index('rege')\n [['loss_tr','loss_dev']]\n .plot()\n)\n\n\n\n\n\n\n\n\nLet’s identify the regularization that leads to the lowest loss on the dev set.\n\ndf.loss_dev.idxmin(), df.loss_dev.min()\n\n(66, 2.24414324760437)\n\n\n\ndf.iloc[df.loss_dev.idxmin(), :]\n\nrege       -2.341709\nreg         0.004553\nloss_tr     2.233218\nloss_dev    2.244143\nName: 66, dtype: float64\n\n\nRetrain with this best value of regularization\n\n10**(rege[df.loss_dev.idxmin()].item())\n\n0.004552933843974289\n\n\n\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((num_chars*num_chars,num_chars), generator=g, requires_grad=True) #single layer of 27 neurons each getting 27x27 inputs\nreg = 10**(rege[df.loss_dev.idxmin()].item())\nfor k in range(400):\n    xs, ys = Xtr, Ytr\n    xenc = F.one_hot(xs, num_classes = num_chars*num_chars).float()\n    logits = xenc@W #log-counts\n    counts = logits.exp() # exponentiate the logits to get fake counts\n    probs = counts/counts.sum(1,keepdims=True)\n    \n    loss_1 = (-(probs[torch.arange(xs.nelement()),ys]).log()).mean()\n    loss_2 = reg*(W**2).mean() #regularization loss\n    \n    loss = loss_1 + loss_2\n\n    # if k%40==0: print(loss.item())\n    \n    #backward pass\n    W.grad = None #More efficient than setting to zero directly. Lack of gradient is interpreted as zero by PyTorch\n    loss.backward()\n    \n    #update\n    W.data += -4*50 * W.grad\nprint(loss_1.item(), loss_2.item(), loss.item())\n\n2.2332184314727783 0.006634071934968233 2.2398524284362793\n\n\nFinally let’s evaluate the loss on the test set\n\nxs, ys = Xte, Yte\nxenc = F.one_hot(xs, num_classes = num_chars*num_chars).float()\nlogits = xenc@W #log-counts\ncounts = logits.exp() # exponentiate the logits to get fake counts\nprobs = counts/counts.sum(1,keepdims=True)\nloss = (-(probs[torch.arange(xs.nelement()),ys]).log()).mean()\nprint(loss.item())\n\n2.2439746856689453",
    "crumbs": [
      "L01_E03"
    ]
  },
  {
    "objectID": "build_makemore_mlp_yay.html",
    "href": "build_makemore_mlp_yay.html",
    "title": "Build makemore MLP",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nwords = open('../names.txt','r').read().splitlines()\nwords[:8]\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\nlen(words)\n\n32033",
    "crumbs": [
      "Build makemore MLP"
    ]
  },
  {
    "objectID": "build_makemore_mlp_yay.html#build-the-dataset",
    "href": "build_makemore_mlp_yay.html#build-the-dataset",
    "title": "Build makemore MLP",
    "section": "Build the dataset",
    "text": "Build the dataset\n\nchars = sorted(set(list(''.join(words))))\nstoi = {char:idx+1 for idx,char in enumerate(chars)}\nstoi['.'] = 0 \nitos = {idx:char for char, idx in stoi.items()}\nprint(itos)\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n\n\n\nblock_size = 3 #characters to take as context before predicting the next\n\nX,Y=[],[]\nfor w in words[:5]:\n    print(w)\n    context = [0]*block_size\n    for ch in w + '.':\n        ix=stoi[ch]\n        X.append(context)\n        Y.append(ix)\n        print(''.join(itos[i] for i in context))\n        context = context[1:] + [ix]\n        \nX = torch.tensor(X)\nY = torch.tensor(Y)\n\nemma\n...\n..e\n.em\nemm\nmma\nolivia\n...\n..o\n.ol\noli\nliv\nivi\nvia\nava\n...\n..a\n.av\nava\nisabella\n...\n..i\n.is\nisa\nsab\nabe\nbel\nell\nlla\nsophia\n...\n..s\n.so\nsop\noph\nphi\nhia\n\n\n\nX.shape, X.dtype, Y.shape, Y.dtype\n\n(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)\n\n\n\n# We will embed the 27 characters into 2-d space\nC = torch.randn((27,2))\n\n\nC[5]\n\ntensor([0.7707, 0.4602])\n\n\n\nemb = C[X]\nemb.shape\n\ntorch.Size([32, 3, 2])\n\n\n\nW1 = torch.randn((6,100)) # 100 neurons each taking 6 inputs\nb1 = torch.randn((100)) # the bias for each of the 100 neurons\n\n\nW2 = torch.randn((100,27)) # 27 neurons each taking 100 inputs\nb2 = torch.randn(27) # the biases for these neurons\n\n\nlogits = h @ W2 + b2\n\n\n#hide\nlogits\n\ntensor([[-1.5712e+00,  6.4775e+00,  2.4227e+00, -6.1163e+00, -9.6750e-01,\n         -3.7930e+00,  3.0395e+00, -4.9640e-02, -4.6716e+00,  7.2450e+00,\n          4.3544e+00,  8.6875e+00,  1.1845e+01, -7.6275e+00,  9.6153e+00,\n          9.5495e-01,  6.8542e+00,  5.7101e+00, -5.0930e+00, -5.1109e+00,\n         -6.6592e+00,  1.5107e+00, -2.7422e+00, -1.0639e+01,  1.1445e+01,\n          6.0562e+00, -7.9084e+00],\n        [-1.1414e+01, -5.2216e+00,  1.4604e+01, -2.9679e+00, -3.2249e+00,\n          5.2879e+00,  1.0380e+00,  3.0865e+00, -2.1076e+00, -1.1681e+00,\n          3.7193e+00,  8.6231e+00,  1.4643e+01, -7.5952e+00,  4.4017e+00,\n          2.7152e+00,  4.0825e+00,  1.9799e-01, -5.1976e+00, -1.1109e+01,\n         -2.2308e+00,  1.4080e+01,  4.4385e-01, -1.1411e+01,  1.4374e+01,\n          4.7474e+00,  1.6334e+00],\n        [ 2.7240e+00,  2.2875e+00,  2.0797e+00, -1.4459e+00,  1.9813e+00,\n         -1.2591e+00,  1.7075e+00,  8.4354e+00,  6.0224e-01,  6.6338e+00,\n         -7.4939e-01,  6.4123e+00,  8.8422e-01, -8.3480e+00,  2.6986e+00,\n         -4.8461e+00, -2.3606e+00,  3.8973e+00, -5.1208e+00, -6.9224e+00,\n         -5.1280e+00,  2.9414e+00, -1.2090e+00, -5.2098e+00,  1.2319e+01,\n          7.9001e+00, -1.0687e+01],\n        [ 2.6909e+00,  3.4520e-01, -4.3895e+00, -1.1090e+01, -5.3512e-01,\n         -5.1047e+00, -1.3574e+01,  3.6265e+00,  3.1571e+00,  8.1377e+00,\n          1.3232e+00,  1.0132e+01,  9.7991e+00, -8.5233e+00,  6.9173e+00,\n         -3.8589e+00,  8.9302e+00, -9.4706e-01, -4.7240e-01, -9.4254e+00,\n         -7.4909e+00,  1.2862e+01, -1.1108e+00, -5.1111e+00,  9.8631e+00,\n         -3.1252e+00, -1.3291e+01],\n        [-8.1137e+00, -1.2994e+01,  1.6356e+01, -4.4418e-01, -3.4179e+00,\n          5.8358e+00, -2.9822e+00,  1.0996e+00,  4.3502e+00, -1.8565e+00,\n          3.0909e+00,  6.3928e+00,  9.9394e+00, -7.4839e+00,  1.8743e+00,\n          5.5036e+00, -2.8334e+00, -7.7502e+00, -1.0118e+01, -1.3051e+01,\n         -2.6067e+00,  1.7873e+01, -5.2643e+00, -8.9650e+00,  1.5337e+01,\n          3.6541e-01, -1.2849e+00],\n        [-1.5712e+00,  6.4775e+00,  2.4227e+00, -6.1163e+00, -9.6750e-01,\n         -3.7930e+00,  3.0395e+00, -4.9640e-02, -4.6716e+00,  7.2450e+00,\n          4.3544e+00,  8.6875e+00,  1.1845e+01, -7.6275e+00,  9.6153e+00,\n          9.5495e-01,  6.8542e+00,  5.7101e+00, -5.0930e+00, -5.1109e+00,\n         -6.6592e+00,  1.5107e+00, -2.7422e+00, -1.0639e+01,  1.1445e+01,\n          6.0562e+00, -7.9084e+00],\n        [ 2.7781e+00,  1.5772e+01, -9.5033e+00, -4.3731e+00,  1.4002e+00,\n         -1.2261e+01,  2.8755e+00,  4.7682e-01, -5.2609e+00,  1.1189e+01,\n          2.7156e+00,  2.8323e+00,  5.3391e+00, -2.2407e+00,  1.4356e+01,\n         -4.1353e+00,  9.5672e+00,  1.1996e+01, -5.7497e-01,  3.5758e+00,\n         -1.0854e+01, -1.0274e+01, -2.1601e+00, -4.4716e+00,  2.9408e-03,\n          8.2284e+00, -1.1229e+01],\n        [-8.3011e+00,  7.8434e+00,  5.4331e-01, -1.1475e+01, -4.0334e+00,\n         -2.9738e-01,  1.0030e+01, -5.3422e+00,  1.8868e+00,  2.1619e-01,\n          5.4946e+00,  9.7572e+00,  1.7324e+01,  2.3993e+00,  7.8914e+00,\n          4.8748e+00,  1.1404e+01,  4.3308e+00,  2.1798e-02, -6.5874e+00,\n          1.8951e+00,  8.1858e-01, -4.0129e+00, -1.3114e+01,  5.2870e+00,\n          1.3965e+00,  8.3496e+00],\n        [-3.2789e+00,  7.0680e+00,  1.7777e+01,  1.4818e+01, -1.0853e+01,\n          5.9951e+00,  8.7378e+00, -3.9229e+00, -1.1113e+01,  5.8286e+00,\n          4.1336e+00,  2.4581e+00,  1.2518e+00, -2.5907e+00,  3.9162e+00,\n          1.1268e+01, -1.7150e+00,  1.2360e+00, -1.9621e+01, -3.3848e+00,\n         -2.3710e-01, -7.0385e-01, -8.7807e+00, -1.1484e+01,  7.7609e+00,\n          5.1029e+00,  5.2459e-01],\n        [ 2.3538e+00,  1.6822e+01, -6.6138e+00, -3.9050e+00,  1.6447e+00,\n         -1.5868e+01,  3.6770e-02, -9.8547e+00, -3.9623e+00,  8.1969e+00,\n          2.4445e+00,  1.1520e+01,  1.2564e+01,  1.8001e+00,  1.4745e+01,\n         -2.0110e+00,  1.0222e+01,  7.6679e+00, -1.0479e+01,  5.4202e+00,\n         -3.9218e+00, -1.6291e+00, -1.0842e+01, -4.2773e+00, -9.5407e-01,\n          9.2356e+00, -4.2587e+00],\n        [-5.5999e+00,  4.6339e-02,  1.3968e+01,  4.2199e-01, -1.3464e+01,\n          3.0614e+00,  4.3031e+00, -3.7535e+00, -4.4443e+00,  5.5488e+00,\n          3.4952e+00, -5.4577e+00,  8.8727e+00, -4.1454e+00,  1.1061e+01,\n          1.0257e+01,  5.6655e+00, -3.4881e+00, -1.2619e+01, -1.5718e+00,\n         -6.2952e+00,  7.7270e+00, -7.6547e-01, -1.2742e+01,  8.5068e+00,\n         -2.6579e+00,  1.0781e+00],\n        [-1.3057e+01, -9.1046e-01,  9.8233e+00,  4.3871e+00, -4.4727e+00,\n          6.5640e-01,  1.2535e+01,  2.3962e+00, -1.2385e+01,  3.0250e+00,\n          1.0686e+01,  9.9788e+00,  1.0376e+01, -1.9435e+00,  8.6010e+00,\n          7.2442e+00,  5.4738e+00,  4.8524e+00, -1.3857e+01, -2.4433e+00,\n          7.0141e+00,  1.0372e+01,  5.8494e-01, -9.3842e+00,  1.2519e+01,\n         -9.1563e-01,  9.4245e+00],\n        [-1.5712e+00,  6.4775e+00,  2.4227e+00, -6.1163e+00, -9.6750e-01,\n         -3.7930e+00,  3.0395e+00, -4.9640e-02, -4.6716e+00,  7.2450e+00,\n          4.3544e+00,  8.6875e+00,  1.1845e+01, -7.6275e+00,  9.6153e+00,\n          9.5495e-01,  6.8542e+00,  5.7101e+00, -5.0930e+00, -5.1109e+00,\n         -6.6592e+00,  1.5107e+00, -2.7422e+00, -1.0639e+01,  1.1445e+01,\n          6.0562e+00, -7.9084e+00],\n        [-1.0373e+01, -9.8450e+00,  1.9715e+01,  1.2702e+00, -4.4900e+00,\n          6.7019e+00,  1.7119e+00, -4.8967e-01,  1.9153e+00, -2.7928e+00,\n          2.4433e+00,  4.7777e+00,  1.1808e+01, -9.5322e+00,  3.8152e+00,\n          7.5374e+00, -1.1621e+00, -5.3773e+00, -1.3271e+01, -9.9735e+00,\n         -2.5263e+00,  1.4034e+01, -4.1221e+00, -1.1579e+01,  1.3921e+01,\n          2.0673e+00,  1.0811e+00],\n        [ 9.8702e+00,  1.5530e+01, -7.1097e+00,  3.2101e+00,  5.0395e+00,\n         -1.1800e+01,  3.1334e+00,  7.9172e-03, -6.7110e+00,  9.4794e+00,\n         -3.6846e-01,  1.3742e+00, -3.0685e+00, -4.8999e+00,  9.8286e+00,\n         -7.6280e+00,  3.1451e-01,  6.6989e+00, -4.5809e+00,  2.9515e+00,\n         -3.3153e+00, -8.9012e+00, -5.9478e+00, -6.2784e+00,  5.0299e+00,\n          1.1671e+01, -1.4521e+01],\n        [-6.8192e+00, -5.1056e+00,  6.3554e+00, -9.9915e+00, -4.3567e+00,\n          1.8480e+00, -5.9830e+00, -6.7096e+00,  9.0971e+00, -5.9679e+00,\n         -1.4075e-01,  4.6723e+00,  1.7833e+01, -1.6069e+00,  1.3336e+01,\n          5.7863e+00,  1.0103e+01, -5.7125e+00, -1.6441e+00, -1.0888e+01,\n         -5.8458e+00,  2.0883e+01, -7.8556e-01, -5.1193e-01,  7.8525e+00,\n         -1.0348e+01, -1.4689e+00],\n        [-1.5712e+00,  6.4775e+00,  2.4227e+00, -6.1163e+00, -9.6750e-01,\n         -3.7930e+00,  3.0395e+00, -4.9640e-02, -4.6716e+00,  7.2450e+00,\n          4.3544e+00,  8.6875e+00,  1.1845e+01, -7.6275e+00,  9.6153e+00,\n          9.5495e-01,  6.8542e+00,  5.7101e+00, -5.0930e+00, -5.1109e+00,\n         -6.6592e+00,  1.5107e+00, -2.7422e+00, -1.0639e+01,  1.1445e+01,\n          6.0562e+00, -7.9084e+00],\n        [-2.7944e+00,  2.4597e+00,  1.8484e+01,  5.4412e+00, -1.0345e+01,\n          4.1629e+00,  1.5971e+00, -1.4642e+00, -7.0187e+00,  6.2320e+00,\n          8.1636e+00,  2.9863e+00,  7.9548e+00, -9.4059e+00,  9.7565e+00,\n          9.3063e+00,  7.1022e-01, -4.1357e+00, -1.9266e+01, -6.9632e+00,\n         -4.0927e+00,  6.2704e+00, -8.9874e+00, -1.0132e+01,  1.1037e+01,\n         -1.6904e-01, -7.1610e+00],\n        [ 4.1152e+00,  1.6659e+01, -1.0580e+01, -2.8056e+00,  3.0078e+00,\n         -1.5674e+01,  1.6634e+00, -6.5095e+00, -8.7387e-01,  8.8331e+00,\n          6.1359e-02,  8.9786e+00,  1.0379e+01,  3.2012e+00,  1.5875e+01,\n         -5.2932e+00,  1.2630e+01,  9.8720e+00, -3.9225e+00,  6.5532e+00,\n         -2.6431e+00, -8.3695e+00, -9.0747e+00, -3.3724e+00, -3.3914e+00,\n          8.6024e+00, -3.9129e+00],\n        [-5.9882e+00, -7.9256e+00,  1.7311e+01, -2.7060e+00, -8.6274e+00,\n          7.7673e+00,  9.6774e-01, -3.2389e+00,  7.4955e+00, -3.9633e+00,\n         -3.5289e+00, -1.3769e+00,  1.4580e+01, -1.7096e+00,  5.8002e+00,\n          6.6892e+00,  7.2678e+00, -1.0491e+00, -5.1896e+00, -1.1165e+01,\n         -5.4481e+00,  1.2884e+01, -6.9343e-01, -1.4964e+01,  1.0520e+01,\n         -5.9684e-01,  1.9344e+00],\n        [-6.3779e+00,  7.6627e+00,  1.0554e+01,  1.2822e+01, -7.1233e+00,\n          5.5820e+00,  1.3102e+01, -2.2062e+00, -1.2082e+01,  7.2374e+00,\n          4.7888e+00,  4.5431e+00,  4.1381e+00, -5.4054e-01,  3.2053e+00,\n          1.0570e+00, -3.6033e+00,  8.3159e+00, -9.6000e+00, -5.3868e-01,\n         -5.0949e+00, -7.6297e-01, -5.9547e+00, -4.4397e+00,  1.4877e+01,\n          9.6608e+00,  2.2357e+00],\n        [ 2.5070e+00, -6.7642e+00, -3.4144e+00, -7.4065e+00, -5.6143e-01,\n         -3.6607e+00, -1.2330e+01,  6.8767e+00,  6.2150e+00,  7.6221e+00,\n         -5.2523e-01,  9.3214e+00,  9.4947e+00, -1.1244e+01,  5.8591e+00,\n         -2.2351e+00,  1.0364e+01, -2.7705e+00, -7.6708e+00, -7.3663e+00,\n         -1.8132e+00,  2.2463e+01,  1.9395e+00, -4.9492e+00,  6.8813e+00,\n         -4.7338e+00, -1.0243e+01],\n        [ 6.2369e+00, -4.2954e-01, -3.8479e+00, -6.9674e+00, -2.9983e+00,\n         -3.2102e+00, -5.7770e+00,  7.9880e+00,  7.7360e+00,  7.7267e+00,\n         -6.4441e+00,  3.4970e+00, -1.0163e+00, -1.2489e+01,  4.1478e+00,\n         -8.1538e+00,  1.9925e+00,  7.3668e+00, -5.0776e+00, -1.1681e+01,\n         -2.8693e+00,  4.9756e+00,  2.8869e+00, -7.1886e+00,  7.3547e+00,\n          6.1679e+00, -1.7327e+01],\n        [ 2.0446e+00,  2.1432e+00, -6.3501e+00, -1.1753e+01, -2.8364e-01,\n         -6.4276e+00, -1.2720e+01,  3.4048e+00,  2.4393e+00,  7.7354e+00,\n          1.4673e+00,  1.0444e+01,  1.0206e+01, -6.7567e+00,  7.4967e+00,\n         -5.4738e+00,  1.0771e+01,  7.9474e-01,  3.6025e+00, -9.1816e+00,\n         -8.1699e+00,  1.1093e+01,  1.7845e-01, -4.4326e+00,  9.5199e+00,\n         -3.9068e+00, -1.1582e+01],\n        [-8.7417e+00, -1.3035e+01,  1.8533e+01, -3.8315e-01, -4.2558e+00,\n          6.3288e+00,  1.4208e-01,  4.6246e-01,  3.9653e+00, -2.0584e+00,\n          2.6605e+00,  5.8512e+00,  9.7596e+00, -6.0193e+00,  7.6110e-01,\n          7.3024e+00, -4.3665e+00, -9.6208e+00, -9.2324e+00, -1.3271e+01,\n         -2.0581e+00,  1.5783e+01, -6.8011e+00, -9.4961e+00,  1.4576e+01,\n          1.8220e+00,  1.0937e+00],\n        [-1.5712e+00,  6.4775e+00,  2.4227e+00, -6.1163e+00, -9.6750e-01,\n         -3.7930e+00,  3.0395e+00, -4.9640e-02, -4.6716e+00,  7.2450e+00,\n          4.3544e+00,  8.6875e+00,  1.1845e+01, -7.6275e+00,  9.6153e+00,\n          9.5495e-01,  6.8542e+00,  5.7101e+00, -5.0930e+00, -5.1109e+00,\n         -6.6592e+00,  1.5107e+00, -2.7422e+00, -1.0639e+01,  1.1445e+01,\n          6.0562e+00, -7.9084e+00],\n        [ 2.3566e+00,  1.3321e+01, -1.0673e+01, -5.0367e+00,  2.1816e+00,\n         -1.3539e+01,  2.3319e+00,  1.6204e+00, -3.1216e+00,  1.1419e+01,\n          2.5245e+00,  4.2840e+00,  5.4702e+00, -2.3846e+00,  1.3813e+01,\n         -7.0136e+00,  9.7795e+00,  1.3572e+01,  1.5561e+00,  5.8909e-01,\n         -1.1135e+01, -1.1477e+01, -2.6126e+00, -4.1688e+00,  4.3893e-01,\n          7.8102e+00, -1.0677e+01],\n        [-5.0249e+00,  1.3887e+01, -1.0441e+01, -5.1595e+00,  4.5026e+00,\n         -1.1167e+01,  1.6500e+00, -4.5957e+00, -6.6368e+00,  6.7433e+00,\n          1.7717e+00,  3.6393e+00,  1.6428e+01,  2.3261e+00,  1.5833e+01,\n         -9.4666e-01,  1.6367e+01,  9.7090e+00,  2.9183e-01, -1.2338e+00,\n         -9.6955e+00, -9.5317e+00, -6.9675e+00, -5.4574e+00, -5.5669e+00,\n          5.4365e+00, -8.7264e-01],\n        [-1.2487e+01,  3.5623e+00,  1.5455e+01, -3.2870e+00, -3.4289e+00,\n          4.7080e-01,  2.0155e+01,  1.7393e+00, -3.4120e+00, -3.2637e+00,\n          3.6447e+00,  8.1095e+00,  1.5873e+01,  5.8974e+00,  3.0125e+00,\n          6.9951e+00,  4.8949e+00, -4.6465e-01, -1.8188e+00,  2.4302e-01,\n          5.3591e+00, -1.5366e+00, -3.2626e+00, -1.3932e+01,  3.1506e+00,\n          9.0332e+00,  1.6305e+01],\n        [-1.1939e+01, -1.9967e+00,  5.0661e+00,  4.3060e-01, -3.7668e+00,\n          1.0969e+01,  1.4923e+01,  1.3497e+01, -4.1815e+00, -1.2281e+00,\n          4.3371e+00,  1.0300e+01, -1.9726e+00,  4.8793e+00, -1.3060e+00,\n          4.8113e-01,  6.8805e+00,  9.2320e+00,  7.4918e+00, -5.2851e+00,\n         -3.6287e+00,  6.0335e+00,  1.5578e+01,  1.8301e+00,  1.3133e+01,\n          1.7611e+00,  1.1575e+01],\n        [-6.2679e+00, -4.6154e+00,  1.2012e+01,  9.7023e-01, -7.1178e+00,\n          1.4591e+01, -1.1914e+01,  5.3174e+00, -1.4772e+00,  4.9499e+00,\n          5.5290e+00, -3.0455e+00,  1.8301e-01, -4.7488e+00, -2.1558e-01,\n          1.6506e+00, -3.8448e-01, -9.4194e-01, -1.2552e+01, -1.2578e+01,\n         -4.8842e+00,  1.1857e+01,  3.1211e+00, -7.6543e+00,  1.2344e+01,\n         -8.5049e+00, -1.7395e+01],\n        [ 9.0097e+00, -3.0882e+00,  1.5451e+00, -5.3382e-01,  6.4750e+00,\n         -3.0595e+00,  2.2897e+00,  2.8895e+00,  2.9219e+00, -1.7165e+00,\n          4.9315e+00,  9.3390e+00,  1.6766e+01,  7.7421e+00,  6.7818e+00,\n         -2.8861e+00,  8.4345e+00, -1.4097e+01, -1.2662e+01, -5.5417e+00,\n          7.2831e-01,  1.8148e+01, -1.5886e+01, -3.4941e-01,  2.0409e+00,\n         -4.2794e+00, -6.5155e+00]])\n\n\n\ncounts = logits.exp()\n\n\nprob = counts/counts.sum(dim=1,keepdim=True)\n\n\nloss = -prob[torch.arange(32),Y].log().mean()\nloss\n\ntensor(15.1157)\n\n\n\n#hide\nprobs = counts/counts.sum()\nprobs\n\ntensor([0., 0., 0., nan])",
    "crumbs": [
      "Build makemore MLP"
    ]
  },
  {
    "objectID": "build_makemore_mlp_yay.html#how-to-determine-a-reasonable-learning-rate",
    "href": "build_makemore_mlp_yay.html#how-to-determine-a-reasonable-learning-rate",
    "title": "Build makemore MLP",
    "section": "How to determine a reasonable learning rate",
    "text": "How to determine a reasonable learning rate\n\nlrei = []\nlossi = []\nfor i in range(1000):\n    #  minibatch construct\n    ix = torch.randint(0,X.shape[0],(32,))\n    # forward pass\n    emb = C[X[ix]] #(32,3,2)\n    h = torch.tanh(emb.view((-1,6)) @ W1 + b1) #(32,100)\n    logits = h @ W2 + b2 #(32,27)\n    loss = F.cross_entropy(logits, Y[ix])\n    print(loss.item())\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()  \n    lr = lrs[i]\n    for p in parameters:\n        p.data += -lr * p.grad\n        \n    #track stats\n    lrei.append(lre[i])\n    lossi.append(loss.item())\n        \n# print(loss.item())\n\n17.57276153564453\n18.38096809387207\n17.454505920410156\n21.21564292907715\n21.080862045288086\n19.18370246887207\n20.907495498657227\n17.437274932861328\n20.813051223754883\n17.57678985595703\n17.47332000732422\n18.42458724975586\n19.1214599609375\n19.37567901611328\n17.27273941040039\n16.733753204345703\n19.102767944335938\n16.82353973388672\n17.026931762695312\n17.733966827392578\n18.997385025024414\n18.89539337158203\n19.080354690551758\n21.113134384155273\n17.584501266479492\n18.75019645690918\n16.487337112426758\n17.951486587524414\n20.475780487060547\n17.734874725341797\n17.48705291748047\n16.38652801513672\n19.465002059936523\n20.70984649658203\n20.237873077392578\n19.666889190673828\n15.294360160827637\n16.641807556152344\n18.040142059326172\n18.168106079101562\n17.376449584960938\n14.23971939086914\n18.829219818115234\n18.401643753051758\n20.517444610595703\n16.9419002532959\n19.847139358520508\n20.43134880065918\n16.74691390991211\n18.466197967529297\n16.289216995239258\n19.889942169189453\n14.608063697814941\n19.21075439453125\n18.152891159057617\n17.74231719970703\n15.69858169555664\n19.93450927734375\n18.348236083984375\n18.120220184326172\n17.106590270996094\n16.182723999023438\n17.517831802368164\n17.5632266998291\n18.986392974853516\n17.822465896606445\n18.883325576782227\n15.437803268432617\n20.238292694091797\n16.066408157348633\n16.811321258544922\n17.691919326782227\n17.516746520996094\n17.40683937072754\n16.440587997436523\n14.209866523742676\n19.31875228881836\n16.58809471130371\n19.984298706054688\n16.213682174682617\n20.02408790588379\n17.488094329833984\n17.630659103393555\n18.153959274291992\n14.730700492858887\n17.322917938232422\n17.27118682861328\n17.030460357666016\n15.972298622131348\n15.829883575439453\n17.603626251220703\n14.789925575256348\n17.900957107543945\n16.318429946899414\n15.928333282470703\n16.756587982177734\n18.634824752807617\n17.96367073059082\n16.790319442749023\n17.987300872802734\n15.947249412536621\n15.854249000549316\n16.925750732421875\n14.131688117980957\n12.612247467041016\n16.975481033325195\n15.191967964172363\n16.70482063293457\n15.168463706970215\n17.52662467956543\n15.80912971496582\n15.530524253845215\n17.074796676635742\n15.531835556030273\n13.902630805969238\n14.349334716796875\n16.51013946533203\n16.840932846069336\n14.3120756149292\n16.94095230102539\n16.750511169433594\n16.597827911376953\n15.281488418579102\n15.984444618225098\n15.352628707885742\n15.726953506469727\n15.2005033493042\n17.539989471435547\n15.83510971069336\n13.191581726074219\n17.930709838867188\n14.170339584350586\n14.35737133026123\n12.1624174118042\n14.047713279724121\n16.080612182617188\n14.233263969421387\n13.770620346069336\n15.541193962097168\n16.58687973022461\n12.992834091186523\n14.201470375061035\n17.04505157470703\n16.81568717956543\n15.340707778930664\n17.045330047607422\n16.104442596435547\n13.92197036743164\n10.169665336608887\n15.461877822875977\n15.259256362915039\n15.423388481140137\n14.177938461303711\n12.693208694458008\n14.739204406738281\n13.535104751586914\n12.780099868774414\n14.464563369750977\n14.233803749084473\n14.992647171020508\n13.311644554138184\n15.53928279876709\n12.041337013244629\n13.274005889892578\n13.575145721435547\n14.164610862731934\n10.109086990356445\n11.826385498046875\n15.98048210144043\n16.12906265258789\n11.293645858764648\n13.293030738830566\n12.55268669128418\n15.317923545837402\n11.3443021774292\n13.8986177444458\n14.035137176513672\n14.727763175964355\n13.629745483398438\n13.641192436218262\n13.92994499206543\n14.99039077758789\n12.029653549194336\n12.228535652160645\n15.812777519226074\n11.198408126831055\n13.486011505126953\n10.95328140258789\n13.02483081817627\n14.279275894165039\n13.457911491394043\n11.409870147705078\n12.480180740356445\n12.73888111114502\n13.069488525390625\n14.35262680053711\n12.483602523803711\n11.16051197052002\n12.968238830566406\n13.372964859008789\n13.601683616638184\n11.866826057434082\n11.84825611114502\n10.876786231994629\n11.830960273742676\n12.459259033203125\n16.673175811767578\n13.886874198913574\n11.077066421508789\n12.524163246154785\n15.63658618927002\n13.755983352661133\n13.191400527954102\n12.109964370727539\n10.664148330688477\n14.248788833618164\n10.48939037322998\n11.84567642211914\n11.79554271697998\n12.221298217773438\n11.716007232666016\n12.102666854858398\n10.648828506469727\n11.631942749023438\n13.382360458374023\n9.633101463317871\n12.316384315490723\n10.129083633422852\n12.461018562316895\n13.69039535522461\n10.605690956115723\n11.078442573547363\n10.647404670715332\n13.907801628112793\n9.22032356262207\n12.12643814086914\n10.30738639831543\n12.606876373291016\n9.222529411315918\n11.954773902893066\n10.49276065826416\n14.304624557495117\n11.358744621276855\n8.687580108642578\n10.281694412231445\n10.74123764038086\n9.27637004852295\n10.500677108764648\n9.315572738647461\n9.520061492919922\n11.103501319885254\n11.161638259887695\n11.23027229309082\n7.806924343109131\n13.278614044189453\n11.83285903930664\n9.629695892333984\n10.773478507995605\n9.072622299194336\n9.812824249267578\n13.24282169342041\n10.963342666625977\n9.725305557250977\n9.029012680053711\n9.03695011138916\n10.131400108337402\n13.947558403015137\n11.419244766235352\n11.898691177368164\n11.660931587219238\n10.061429023742676\n11.497520446777344\n12.809901237487793\n10.863630294799805\n8.125093460083008\n7.469451427459717\n10.671889305114746\n11.224878311157227\n9.64244556427002\n9.643403053283691\n9.454347610473633\n8.55276870727539\n9.348222732543945\n10.299896240234375\n9.231563568115234\n9.949894905090332\n8.077455520629883\n9.404560089111328\n10.465301513671875\n9.281176567077637\n8.108887672424316\n9.470197677612305\n11.760165214538574\n6.930253505706787\n10.075078964233398\n8.409323692321777\n7.802064895629883\n8.755902290344238\n9.555486679077148\n9.077010154724121\n8.427393913269043\n8.506288528442383\n7.170266151428223\n7.758419036865234\n10.807315826416016\n8.01258373260498\n8.218811988830566\n8.3101224899292\n9.369029998779297\n9.459511756896973\n9.708526611328125\n7.758353233337402\n9.022072792053223\n7.03503942489624\n6.063441276550293\n9.579068183898926\n8.003768920898438\n9.531412124633789\n7.2605695724487305\n8.974498748779297\n9.40772533416748\n9.006237030029297\n8.69849681854248\n8.23283863067627\n10.0516939163208\n6.019401550292969\n6.461541652679443\n9.925166130065918\n9.101470947265625\n9.976348876953125\n5.607675075531006\n9.050604820251465\n8.00273323059082\n8.5353422164917\n7.544510841369629\n7.506710052490234\n6.829733848571777\n8.613126754760742\n8.592612266540527\n8.192902565002441\n6.942233085632324\n7.856372833251953\n9.070555686950684\n8.64041519165039\n7.913468837738037\n6.574246406555176\n7.75071907043457\n7.057945251464844\n7.3872971534729\n7.875763893127441\n6.213350296020508\n6.883249759674072\n7.580850601196289\n7.125817775726318\n9.91046142578125\n5.6778435707092285\n6.802744388580322\n6.988166332244873\n8.872272491455078\n7.118231296539307\n8.424520492553711\n7.636692047119141\n9.565744400024414\n8.37964153289795\n7.063146114349365\n6.927211761474609\n4.88486909866333\n5.06980037689209\n6.780410289764404\n6.086301803588867\n4.850202560424805\n7.979799270629883\n6.148624420166016\n5.901304721832275\n5.411088943481445\n6.302879333496094\n6.618958950042725\n4.880417823791504\n5.591373920440674\n6.40765380859375\n5.91107702255249\n6.5665130615234375\n5.255655288696289\n5.528822422027588\n5.648355484008789\n7.044066429138184\n7.250743865966797\n5.330826282501221\n4.594923973083496\n6.730301856994629\n4.8512959480285645\n6.047122478485107\n6.349576950073242\n5.296410083770752\n6.114524841308594\n5.366572380065918\n5.697829246520996\n6.239436149597168\n5.836843490600586\n4.88054895401001\n6.1162190437316895\n5.476934432983398\n6.4069952964782715\n5.5481858253479\n6.545391082763672\n4.72417688369751\n4.981783866882324\n4.559980392456055\n5.38161039352417\n3.975733518600464\n5.603244781494141\n5.809412479400635\n5.7475104331970215\n6.096259593963623\n5.84445858001709\n5.359170436859131\n5.442815780639648\n4.635664463043213\n4.976276874542236\n5.579714775085449\n5.920684337615967\n5.080137252807617\n4.3282694816589355\n4.686100006103516\n4.749856472015381\n4.3419880867004395\n4.526195049285889\n6.3921098709106445\n4.087148189544678\n4.322854042053223\n5.063938617706299\n4.79725456237793\n6.3536272048950195\n5.09281063079834\n4.247463226318359\n3.7620999813079834\n4.812386512756348\n3.9307312965393066\n4.209801197052002\n3.80802845954895\n4.269464492797852\n4.941763877868652\n4.4933600425720215\n4.983832359313965\n5.1211724281311035\n5.657429218292236\n4.3082356452941895\n4.365445137023926\n3.9196009635925293\n4.211538791656494\n5.5622639656066895\n4.30189847946167\n4.318421840667725\n4.036620140075684\n4.024697303771973\n3.599339246749878\n5.080698013305664\n2.981902599334717\n3.6235415935516357\n4.768585205078125\n3.9881162643432617\n3.4466307163238525\n4.302891254425049\n3.896972417831421\n4.975249767303467\n3.3723886013031006\n4.380087375640869\n3.6773946285247803\n3.507293701171875\n3.852163791656494\n4.447554588317871\n4.145574569702148\n4.51737117767334\n4.3943071365356445\n3.8847599029541016\n3.0491714477539062\n3.3182928562164307\n3.145413875579834\n3.453437089920044\n4.861083984375\n5.043808460235596\n4.37673807144165\n4.264418125152588\n3.3205738067626953\n3.5635311603546143\n4.186126232147217\n3.17484450340271\n3.2935609817504883\n4.139324188232422\n4.10752534866333\n4.583308219909668\n3.824875831604004\n4.234518051147461\n3.5373566150665283\n3.6325836181640625\n4.474077224731445\n3.5913314819335938\n3.3791487216949463\n3.818492889404297\n3.576718807220459\n3.074841022491455\n4.589286804199219\n4.9279303550720215\n3.2122926712036133\n3.53069806098938\n3.0408215522766113\n3.1210379600524902\n3.972811460494995\n2.820545196533203\n2.9753661155700684\n3.2039246559143066\n4.304561614990234\n2.795058488845825\n3.6055233478546143\n4.349609375\n4.457000732421875\n3.3578226566314697\n2.853200912475586\n4.547207832336426\n3.71525239944458\n3.8805532455444336\n3.2830116748809814\n3.1410460472106934\n4.244385719299316\n3.1633644104003906\n2.8494255542755127\n2.554918050765991\n3.5575625896453857\n3.138493299484253\n4.082805633544922\n3.247215747833252\n3.0268959999084473\n4.076394081115723\n3.4896204471588135\n3.368285894393921\n3.1239986419677734\n3.4116039276123047\n3.2818264961242676\n3.112525701522827\n3.3597795963287354\n3.598848342895508\n2.900707721710205\n3.9728283882141113\n2.898958206176758\n3.9185235500335693\n3.173433303833008\n3.447232961654663\n3.1679890155792236\n4.447990417480469\n3.107189655303955\n4.22524881362915\n3.3233840465545654\n3.550227642059326\n4.078894138336182\n2.816941261291504\n3.2710134983062744\n4.103876113891602\n3.376629114151001\n3.658601760864258\n3.0675745010375977\n3.127713441848755\n2.9720544815063477\n3.3234899044036865\n3.2029147148132324\n4.110997200012207\n3.207146167755127\n3.7683496475219727\n3.8766274452209473\n2.75616192817688\n3.389221668243408\n3.264263868331909\n3.865874767303467\n2.8424975872039795\n3.7502903938293457\n2.9777672290802\n3.3945934772491455\n3.335766553878784\n3.771994113922119\n3.5832512378692627\n2.956080675125122\n3.4376795291900635\n3.998025894165039\n2.7311813831329346\n3.2158689498901367\n3.540562629699707\n3.2342236042022705\n3.8305037021636963\n3.1390128135681152\n3.485523223876953\n3.4669792652130127\n3.767796754837036\n3.5047719478607178\n2.984759569168091\n3.2164902687072754\n3.1166152954101562\n3.415897846221924\n2.611834764480591\n3.862666130065918\n3.0787580013275146\n3.285639524459839\n4.495078086853027\n3.364108085632324\n3.3341071605682373\n3.744595766067505\n2.965789556503296\n3.279768466949463\n3.3111510276794434\n3.7595818042755127\n3.816115379333496\n4.076173782348633\n4.1771345138549805\n3.138631820678711\n3.277111530303955\n3.956434726715088\n3.6578469276428223\n4.299097061157227\n4.024972438812256\n3.4792001247406006\n4.518208980560303\n3.919412136077881\n4.384096145629883\n4.809706211090088\n4.574504375457764\n4.825948715209961\n3.8937370777130127\n3.2497544288635254\n3.348829746246338\n3.877976655960083\n4.807061195373535\n4.19301700592041\n3.7928073406219482\n3.5153565406799316\n4.369344711303711\n4.746561050415039\n5.9832940101623535\n6.059764385223389\n5.890462875366211\n3.8835947513580322\n3.609372854232788\n5.062462329864502\n4.113190650939941\n3.581843852996826\n3.409597396850586\n4.5961103439331055\n4.65641450881958\n4.902923107147217\n4.033980369567871\n4.058071136474609\n4.980954647064209\n4.085536956787109\n4.229255676269531\n3.293346643447876\n3.464874267578125\n3.9673638343811035\n3.923771381378174\n3.278610944747925\n4.934590816497803\n6.165642738342285\n4.442200660705566\n4.853152751922607\n4.61954927444458\n3.7030282020568848\n4.485597610473633\n5.032050609588623\n4.393487930297852\n4.561595439910889\n3.8683595657348633\n4.462499618530273\n4.602001667022705\n5.785060405731201\n5.904040336608887\n5.32254695892334\n4.12208366394043\n4.970413684844971\n6.646961212158203\n6.090055465698242\n4.996232032775879\n5.3743205070495605\n4.901844024658203\n3.9967503547668457\n5.7420973777771\n6.997772216796875\n6.238030910491943\n3.924320697784424\n5.6335883140563965\n5.382786273956299\n5.185755729675293\n5.015298843383789\n5.182074546813965\n6.259066581726074\n5.735254287719727\n8.402029037475586\n7.44135046005249\n8.499380111694336\n6.106293201446533\n6.0944414138793945\n5.301347255706787\n8.861555099487305\n12.311391830444336\n7.2152557373046875\n6.250033855438232\n5.284031867980957\n5.381573677062988\n4.5272369384765625\n6.995848655700684\n7.101648330688477\n7.960639953613281\n6.852879047393799\n3.8690097332000732\n4.072272300720215\n5.209026336669922\n5.6040520668029785\n5.993564605712891\n7.087926864624023\n8.062431335449219\n9.747995376586914\n6.790176868438721\n7.22239351272583\n5.771853923797607\n5.434926509857178\n7.397158145904541\n7.73043966293335\n6.790462493896484\n8.449735641479492\n6.94552755355835\n10.914224624633789\n9.567365646362305\n10.504263877868652\n6.578146457672119\n7.8354668617248535\n8.277453422546387\n8.893840789794922\n11.445467948913574\n6.639841556549072\n7.730043411254883\n8.568772315979004\n7.460530757904053\n7.207331657409668\n8.167706489562988\n7.838566303253174\n5.800595283508301\n10.115594863891602\n4.292886734008789\n8.438791275024414\n7.931124687194824\n6.343996047973633\n8.375015258789062\n6.468666076660156\n6.902678489685059\n9.411162376403809\n10.597082138061523\n10.699217796325684\n11.059724807739258\n7.664960861206055\n7.898968696594238\n6.15482759475708\n6.096246719360352\n6.936249732971191\n7.655722141265869\n9.280254364013672\n6.194483280181885\n8.092377662658691\n8.768957138061523\n7.386622905731201\n10.86899471282959\n10.410085678100586\n10.68460750579834\n8.769866943359375\n8.722225189208984\n9.647274017333984\n7.9557600021362305\n8.581648826599121\n7.340024948120117\n6.9809417724609375\n8.761804580688477\n8.999138832092285\n10.290088653564453\n6.927513599395752\n9.826526641845703\n11.084794044494629\n6.962419509887695\n11.296157836914062\n8.386797904968262\n9.349197387695312\n9.952279090881348\n10.190406799316406\n13.448662757873535\n10.416336059570312\n10.262770652770996\n11.59829330444336\n8.883024215698242\n10.651888847351074\n8.286900520324707\n10.57921314239502\n10.578049659729004\n11.571516990661621\n12.024585723876953\n8.149734497070312\n11.033391952514648\n9.977598190307617\n11.880741119384766\n8.24068832397461\n9.956643104553223\n11.348530769348145\n14.568349838256836\n16.258228302001953\n12.204757690429688\n10.767562866210938\n12.151226043701172\n14.850882530212402\n12.264532089233398\n10.204282760620117\n10.683204650878906\n12.85846996307373\n17.945899963378906\n10.569111824035645\n14.298750877380371\n10.739777565002441\n17.308917999267578\n13.745304107666016\n11.550067901611328\n12.289779663085938\n11.560111045837402\n11.181889533996582\n12.558286666870117\n11.133200645446777\n11.198123931884766\n12.016657829284668\n11.244430541992188\n10.727561950683594\n13.395211219787598\n10.224061012268066\n12.716102600097656\n14.471595764160156\n13.401673316955566\n17.21458625793457\n14.323077201843262\n19.27243423461914\n20.000822067260742\n16.914438247680664\n9.531935691833496\n12.516231536865234\n12.459766387939453\n9.207915306091309\n10.808636665344238\n10.807703971862793\n12.781746864318848\n14.399681091308594\n15.479743003845215\n12.067764282226562\n16.33179473876953\n16.757856369018555\n15.209238052368164\n18.29767608642578\n17.13522720336914\n15.591724395751953\n14.667969703674316\n15.0635347366333\n15.324156761169434\n15.345002174377441\n13.614517211914062\n11.576675415039062\n19.428817749023438\n15.338082313537598\n15.292872428894043\n13.03862190246582\n12.927718162536621\n13.992708206176758\n31.782779693603516\n14.408294677734375\n16.602188110351562\n27.092931747436523\n20.221508026123047\n19.14093780517578\n16.108354568481445\n17.73938751220703\n16.296924591064453\n20.588321685791016\n16.90170669555664\n17.348058700561523\n17.156356811523438\n26.146474838256836\n23.42630386352539\n21.714170455932617\n25.21057891845703\n28.225736618041992\n26.44867515563965\n24.209041595458984\n24.74105453491211\n31.595918655395508\n21.057050704956055\n23.551347732543945\n21.69952964782715\n20.37238883972168\n23.448055267333984\n30.25034523010254\n21.83735466003418\n23.406211853027344\n25.50998306274414\n22.3135929107666\n32.62458801269531\n38.18891143798828\n23.028343200683594\n25.610549926757812\n22.401121139526367\n24.220216751098633\n32.993831634521484\n27.283489227294922\n31.83912467956543\n32.94308090209961\n34.9365234375\n27.782106399536133\n29.326738357543945\n41.74710464477539\n23.350324630737305\n26.150951385498047\n34.16910171508789\n36.40987777709961\n37.45579528808594\n20.85873794555664\n43.587379455566406\n39.335025787353516\n27.194637298583984\n28.844467163085938\n38.47759246826172\n38.109718322753906\n32.55181121826172\n26.55961036682129\n35.65530014038086\n27.396835327148438\n34.551090240478516\n26.080490112304688\n27.279502868652344\n25.24319076538086\n25.858488082885742\n33.77938461303711\n24.02435302734375\n27.767898559570312\n24.559001922607422\n43.594642639160156\n26.68766975402832\n26.41925621032715\n37.36347198486328\n41.81816101074219\n34.90917205810547\n40.68021011352539\n32.32591247558594\n38.547874450683594\n34.61859893798828\n34.86686325073242\n36.973358154296875\n38.11799240112305\n51.238555908203125\n49.27067565917969\n46.336307525634766\n53.21565246582031\n46.25751495361328\n45.859031677246094\n36.48525619506836\n51.31349563598633\n47.10385513305664\n61.56968307495117\n34.892086029052734\n56.66950988769531\n70.61302185058594\n40.962833404541016\n58.1834831237793\n58.44976806640625\n60.04684829711914\n45.72854995727539\n43.415428161621094\n35.57026290893555\n32.213966369628906\n52.3416748046875\n43.54084396362305\n35.349754333496094\n35.295555114746094\n54.3027229309082\n55.16071319580078\n60.0406608581543\n49.719871520996094\n58.919681549072266\n90.24168395996094\n53.76655197143555\n53.47417449951172\n52.507747650146484\n65.25830078125\n68.51478576660156\n65.1689224243164\n64.79029846191406\n39.27325439453125\n59.22473907470703\n49.49073028564453\n90.13214874267578\n69.495361328125\n57.639835357666016\n69.23587799072266\n94.24659729003906\n83.33028411865234\n64.9979248046875\n\n\n\nblock_size = 3 #characters to take as context before predicting the next\n\nX,Y=[],[]\nfor w in words:\n#     print(w)\n    context = [0]*block_size\n    for ch in w + '.':\n        ix=stoi[ch]\n        X.append(context)\n        Y.append(ix)\n#         print(''.join(itos[i] for i in context))\n        context = context[1:] + [ix]\n        \nX = torch.tensor(X)\nY = torch.tensor(Y)\n\n\ng = torch.Generator().manual_seed(2147483647)\nC = torch.randn((27,2), generator=g)\n\nW1 = torch.randn((6,100), generator=g) # 100 neurons each taking 6 inputs\nb1 = torch.randn((100), generator=g) # the bias for each of the 100 neurons\n\nW2 = torch.randn((100,27), generator=g) # 27 neurons each taking 100 inputs\nb2 = torch.randn(27, generator=g)\n\nparameters = [C, W1, b1, W2, b2]\n\nfor p in parameters:\n    p.requires_grad = True\n\n\nlrei = []\nlossi = []\nfor i in range(20000):\n    #  minibatch construct\n    ix = torch.randint(0,X.shape[0],(32,))\n    # forward pass\n    emb = C[X[ix]] #(32,3,2)\n    h = torch.tanh(emb.view((-1,6)) @ W1 + b1) #(32,100)\n    logits = h @ W2 + b2 #(32,27)\n    loss = F.cross_entropy(logits, Y[ix])\n#     print(loss.item())\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()  \n    lr = 10**-1 #lrs[i]\n    for p in parameters:\n        p.data += -lr * p.grad\n        \n    #track stats\n#     lrei.append(lre[i])\n#     lossi.append(loss.item())\n        \nprint(loss.item())\n\n2.3090484142303467\n\n\nDecay the learning rate\n\nlrei = []\nlossi = []\nfor i in range(10000):\n    #  minibatch construct\n    ix = torch.randint(0,X.shape[0],(32,))\n    # forward pass\n    emb = C[X[ix]] #(32,3,2)\n    h = torch.tanh(emb.view((-1,6)) @ W1 + b1) #(32,100)\n    logits = h @ W2 + b2 #(32,27)\n    loss = F.cross_entropy(logits, Y[ix])\n#     print(loss.item())\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()  \n    lr = 10**-3 #lrs[i]\n    for p in parameters:\n        p.data += -lr * p.grad\n        \n    #track stats\n#     lrei.append(lre[i])\n#     lossi.append(loss.item())\n        \nprint(loss.item())\n\n2.323967933654785\n\n\n\n#build dataset\n\ndef build_dataset(words):\n    block_size = 3 #characters to take as context before predicting the next\n\n    X,Y=[],[]\n    for w in words:\n    #     print(w)\n        context = [0]*block_size\n        for ch in w + '.':\n            ix=stoi[ch]\n            X.append(context)\n            Y.append(ix)\n    #         print(''.join(itos[i] for i in context))\n            context = context[1:] + [ix]\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    \n    print(X.shape, Y.shape)\n    \n    return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,Ytr=build_dataset(words[:n1])\nXdev,Ydev=build_dataset(words[n1:n2])\nXte,Yte=build_dataset(words[n2:])\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n\n\n\ng = torch.Generator().manual_seed(2147483647)\nC = torch.randn((27,2), generator=g)\n\nW1 = torch.randn((6,100), generator=g) # 100 neurons each taking 6 inputs\nb1 = torch.randn((100), generator=g) # the bias for each of the 100 neurons\n\nW2 = torch.randn((100,27), generator=g) # 27 neurons each taking 100 inputs\nb2 = torch.randn(27, generator=g)\n\nparameters = [C, W1, b1, W2, b2]\n\nfor p in parameters:\n    p.requires_grad = True\n\nNow train on Xtr,Ytr\n\nlrei = []\nlossi = []\nfor i in range(30000):\n    #  minibatch construct\n    ix = torch.randint(0,Xtr.shape[0],(32,))\n    # forward pass\n    emb = C[Xtr[ix]] #(32,3,2)\n    h = torch.tanh(emb.view((-1,6)) @ W1 + b1) #(32,100)\n    logits = h @ W2 + b2 #(32,27)\n    loss = F.cross_entropy(logits, Ytr[ix])\n#     print(loss.item())\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()  \n    lr = 10**-1 #lrs[i]\n    for p in parameters:\n        p.data += -lr * p.grad\n        \n    #track stats\n#     lrei.append(lre[i])\n#     lossi.append(loss.item())\n        \nprint(loss.item())\n\n2.3105552196502686\n\n\nDecay the learning rate\n\nlrei = []\nlossi = []\nfor i in range(20000):\n    #  minibatch construct\n    ix = torch.randint(0,Xtr.shape[0],(32,))\n    # forward pass\n    emb = C[Xtr[ix]] #(32,3,2)\n    h = torch.tanh(emb.view((-1,6)) @ W1 + b1) #(32,100)\n    logits = h @ W2 + b2 #(32,27)\n    loss = F.cross_entropy(logits, Ytr[ix])\n#     print(loss.item())\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()  \n    lr = 10**-2 #lrs[i]\n    for p in parameters:\n        p.data += -lr * p.grad\n        \n    #track stats\n#     lrei.append(lre[i])\n#     lossi.append(loss.item())\n        \nprint(loss.item())\n\n2.535987615585327\n\n\nEvaluate on the Dev set\n\nemb = C[Xdev]\nh = torch.tanh(emb.view((-1,6)) @ W1 + b1) #(32,100)\nlogits = h @ W2 + b2\nloss = F.cross_entropy(logits, Ydev)\nprint(loss.item())\n\n2.32991361618042",
    "crumbs": [
      "Build makemore MLP"
    ]
  },
  {
    "objectID": "build_makemore_mlp_yay.html#scale-up-the-neural-net",
    "href": "build_makemore_mlp_yay.html#scale-up-the-neural-net",
    "title": "Build makemore MLP",
    "section": "Scale up the neural net",
    "text": "Scale up the neural net\n\ng = torch.Generator().manual_seed(2147483647)\nC = torch.randn((27,2), generator=g)\n\nW1 = torch.randn((6,300), generator=g) # 300 neurons each taking 6 inputs\nb1 = torch.randn((300), generator=g) # the bias for each of the 300 neurons\n\nW2 = torch.randn((300,27), generator=g) # 27 neurons each taking 300 inputs\nb2 = torch.randn(27, generator=g)\n\nparameters = [C, W1, b1, W2, b2]\n\nprint(sum([p.nelement() for p in parameters]))\n\nfor p in parameters:\n    p.requires_grad = True\n\n10281\n\n\n\nlrei = []\nlossi = []\nstepi = []\nfor i in range(30000):\n    #  minibatch construct\n    ix = torch.randint(0,Xtr.shape[0],(32,))\n    # forward pass\n    emb = C[Xtr[ix]] #(32,3,2)\n    h = torch.tanh(emb.view((-1,6)) @ W1 + b1) #(32,100)\n    logits = h @ W2 + b2 #(32,27)\n    loss = F.cross_entropy(logits, Ytr[ix])\n#     print(loss.item())\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()  \n    lr = 10**-2 #lrs[i]\n    for p in parameters:\n        p.data += -lr * p.grad\n        \n    #track stats\n#     lrei.append(lre[i])\n    lossi.append(loss.item())\n    stepi.append(i)\n        \nprint(loss.item())\n\n2.2436883449554443\n\n\n\nplt.plot(stepi, lossi)\n\n\n\n\n\n\n\n\n\nemb = C[Xdev]\nh = torch.tanh(emb.view((-1,6)) @ W1 + b1) #(32,100)\nlogits = h @ W2 + b2\nloss = F.cross_entropy(logits, Ydev)\nprint(loss.item())\n\n2.228832483291626",
    "crumbs": [
      "Build makemore MLP"
    ]
  },
  {
    "objectID": "build_makemore_mlp_yay.html#visualize-embedding",
    "href": "build_makemore_mlp_yay.html#visualize-embedding",
    "title": "Build makemore MLP",
    "section": "Visualize embedding",
    "text": "Visualize embedding\nObserve how most of the vowels are clustered to the bottom left. q seems to be far and on it’s own\n\nplt.figure(figsize=(8,8))\nplt.scatter(C[:,0].data,C[:,1].data,s=200)\nfor i in range(C.shape[0]):\n    plt.text(C[i,0].item(),C[i,1].item(),itos[i],ha=\"center\",va=\"center\",color=\"white\")\nplt.grid(\"minor\")",
    "crumbs": [
      "Build makemore MLP"
    ]
  },
  {
    "objectID": "build_makemore_mlp_yay.html#lets-make-the-embedding-vector-bigger",
    "href": "build_makemore_mlp_yay.html#lets-make-the-embedding-vector-bigger",
    "title": "Build makemore MLP",
    "section": "Let’s make the embedding vector bigger",
    "text": "Let’s make the embedding vector bigger\nThis may be the bottleneck to getting a better loss.\n\ng = torch.Generator().manual_seed(2147483647)\nC = torch.randn((27,10), generator=g)\n\nW1 = torch.randn((30,200), generator=g) # 200 neurons each taking 30 inputs (10 per character in the context)\nb1 = torch.randn((200), generator=g) # the bias for each of the 200 neurons\n\nW2 = torch.randn((200,27), generator=g) # 27 neurons each taking 200 inputs\nb2 = torch.randn(27, generator=g)\n\nparameters = [C, W1, b1, W2, b2]\n\nprint(sum([p.nelement() for p in parameters]))\n\nfor p in parameters:\n    p.requires_grad = True\n\n11897\n\n\n\nlrei = []\nlossi = []\nstepi = []\n\n\nfor i in range(200000):\n    #  minibatch construct\n    ix = torch.randint(0,Xtr.shape[0],(32,))\n    # forward pass\n    emb = C[Xtr[ix]] #(32,3,2)\n    h = torch.tanh(emb.view((-1,30)) @ W1 + b1) #(32,100)\n    logits = h @ W2 + b2 #(32,27)\n    loss = F.cross_entropy(logits, Ytr[ix])\n#     print(loss.item())\n    #backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()  \n    lr = 10**-1 if i &lt; 100000 else 10**-2 #lrs[i]\n    for p in parameters:\n        p.data += -lr * p.grad\n        \n    #track stats\n#     lrei.append(lre[i])\n    lossi.append(loss.log10().item()) #track log loss for o/w plot has a hockey stick shape\n    stepi.append(i)\n        \nprint(loss.item())\n\n1.8843928575515747\n\n\n\nplt.plot(stepi, lossi)\n\n\n\n\n\n\n\n\n\nemb = C[Xdev]\nh = torch.tanh(emb.view((-1,30)) @ W1 + b1) #(32,100)\nlogits = h @ W2 + b2\nloss = F.cross_entropy(logits, Ydev)\nprint(loss.item())\n\n2.1519885063171387",
    "crumbs": [
      "Build makemore MLP"
    ]
  },
  {
    "objectID": "build_makemore_mlp_yay.html#sample-from-the-model",
    "href": "build_makemore_mlp_yay.html#sample-from-the-model",
    "title": "Build makemore MLP",
    "section": "Sample from the model",
    "text": "Sample from the model\n\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    out=[]\n    context = [0]*block_size\n    while True:\n        emb = C[torch.tensor([context])]\n        h = torch.tanh(emb.view((1,-1)) @ W1 + b1)\n        logits = h @ W2 + b2\n        probs = F.softmax(logits, dim=1)\n        \n        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n        context = context[1:]+[ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    print(''.join([itos[i] for i in out]))\n\ncarmahzari.\nharli.\njorri.\ntaty.\nska.\nsane.\nmahnen.\ndelyah.\njareei.\nner.\nkiah.\nmaiir.\nkaleigh.\nham.\njorn.\nquintis.\nlilea.\njadzi.\nwajerma.\njarysi.",
    "crumbs": [
      "Build makemore MLP"
    ]
  },
  {
    "objectID": "l01_e02.html",
    "href": "l01_e02.html",
    "title": "L01_E02",
    "section": "",
    "text": "Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. Train the bigram and trigram models only on the training set. Evaluate them on dev and test splits. What can you see?\nLet’s do this with the counting model.\nimport torch\nimport random\nrandom.seed(42)\n\nwords = open('../names.txt','r').read().splitlines()\n\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nwords_tr = words[:n1]\nwords_dev = words[n1:n2]\nwords_te = words[n2:]\nchars = sorted(list(set(''.join(words_tr))))\nlen(chars)\n\n26\nctoi = {c : i+1 for i,c in enumerate(chars)}\nctoi['.'] = 0\nitoc = {i:c for c,i in ctoi.items()}\nnum_chars = len(ctoi.keys())\nnum_chars\n\n27",
    "crumbs": [
      "L01_E02"
    ]
  },
  {
    "objectID": "l01_e02.html#model-evaluation",
    "href": "l01_e02.html#model-evaluation",
    "title": "L01_E02",
    "section": "Model evaluation",
    "text": "Model evaluation\n\ndef evaluate_average_negloglikelihood_bigram(words):\n    log_likelihood = 0.\n    n = 0\n    for word in words:\n        chs = '.' + word + '.'\n        for ch1,ch2 in zip(chs,chs[1:]):\n            prob = P[ctoi[ch1],ctoi[ch2]]\n            logprob = torch.log(prob)\n            log_likelihood += logprob\n            n += 1\n    \n    print(f'{log_likelihood=:.4f}') \n    print(f'{n=}')\n    nll = -log_likelihood\n    print(f'{nll/n=:.4f}')\n\n\n# performance on train\nevaluate_average_negloglikelihood_bigram(words_tr)\n\nlog_likelihood=-448229.5938\nn=182625\nnll/n=2.4544\n\n\n\n# performance on dev\nevaluate_average_negloglikelihood_bigram(words_dev)\n\nlog_likelihood=-55579.4883\nn=22655\nnll/n=2.4533\n\n\n\n# performance on test\nevaluate_average_negloglikelihood_bigram(words_te)\n\nlog_likelihood=-56214.3711\nn=22866\nnll/n=2.4584\n\n\nPerformance on the dev and test sets don’t show overfitting.",
    "crumbs": [
      "L01_E02"
    ]
  },
  {
    "objectID": "l01_e02.html#model-evaluation-1",
    "href": "l01_e02.html#model-evaluation-1",
    "title": "L01_E02",
    "section": "Model evaluation",
    "text": "Model evaluation\n\ndef evaluate_average_negloglikelihood_trigram(words):\n    log_likelihood = 0.\n    n = 0\n    for word in words:\n        chs = '..' + word + '.'\n        for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n            #print(ch1,ch2,ch3)\n            prob = P[stoi[ch1,ch2],ctoi[ch3]]\n            logprob = torch.log(prob)\n            log_likelihood += logprob\n            n += 1\n    \n    print(f'{log_likelihood=:.4f}') \n    print(f'{n=}')\n    nll = -log_likelihood\n    print(f'{nll/n=:.4f}')\n\n\nevaluate_average_negloglikelihood_trigram(words_tr)\n\nlog_likelihood=-404643.9062\nn=182625\nnll/n=2.2157\n\n\n\nevaluate_average_negloglikelihood_trigram(words_dev)\n\nlog_likelihood=-50666.9961\nn=22655\nnll/n=2.2365\n\n\n\nevaluate_average_negloglikelihood_trigram(words_te)\n\nlog_likelihood=-51158.6094\nn=22866\nnll/n=2.2373\n\n\nThere seems to be overfitting on the training set. The performance on the dev and test sets are worse than on the training set.",
    "crumbs": [
      "L01_E02"
    ]
  },
  {
    "objectID": "l01_e04.html",
    "href": "l01_e04.html",
    "title": "L01_E04",
    "section": "",
    "text": "We saw that our 1-hot vectors merely select a row of W, so producing these vectors explicitly feels wasteful. Can you delete our use of F.one_hot in favor of simply indexing into rows of W?\n\nimport torch\n\n\nimport random\nrandom.seed(42)\n\nwords = open('../names.txt','r').read().splitlines()\n\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nwords_tr = words[:n1]\nwords_dev = words[n1:n2]\nwords_te = words[n2:]\n\n\nchars = sorted(list(set(''.join(words_tr))))\nlen(chars)\n\n26\n\n\n\nctoi = {c : i+1 for i,c in enumerate(chars)}\nctoi['.'] = 0\n\n\nitoc = {i:c for c,i in ctoi.items()}\n\n\nnum_chars = len(ctoi.keys())\nnum_chars\n\n27\n\n\n\nstoi = {}\nfor i0,c0 in sorted(itoc.items(), key=lambda kv: kv[0]):\n    for i1,c1 in sorted(itoc.items(), key=lambda kv: kv[0]):\n        #print((i0*num_chars) + i1,c0,c1)\n        stoi[(c0,c1)] = (i0*num_chars) + i1\n\n\ndef build_dataset(words):\n    xs,ys = [],[]\n    \n    for word in words:\n        chs = '..' + word + '.'\n        for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n            ix1 = stoi[ch1,ch2]\n            ix2 = ctoi[ch3]\n            xs.append(ix1)\n            ys.append(ix2)\n    \n    # prefer to use torch.tensor instead of torch.Tensor\n    xs = torch.tensor(xs)\n    ys = torch.tensor(ys)\n    num = xs.nelement()\n    print(f'number of examples: {num}')   \n\n    return xs, ys\n\n\nXtr,Ytr=build_dataset(words_tr)\nXdev,Ydev=build_dataset(words_dev)\nXte,Yte=build_dataset(words_te)\n\nnumber of examples: 182625\nnumber of examples: 22655\nnumber of examples: 22866\n\n\n\nXtr.dtype\n\ntorch.int64\n\n\n\nimport torch.nn.functional as F\n\n\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((num_chars*num_chars,num_chars), generator=g, requires_grad=True) #single layer of 27 neurons each getting 27x27 inputs\n\nreg = 0.004552933843974289\nfor k in range(400):\n    xs, ys = Xtr, Ytr\n    logits = W[xs] #log-counts\n    counts = logits.exp() # exponentiate the logits to get fake counts\n    probs = counts/counts.sum(1,keepdims=True)  \n    \n    loss_1 = (-(probs[torch.arange(xs.nelement()),ys]).log()).mean()\n    loss_2 = reg*(W**2).mean() #regularization loss\n    \n    loss = loss_1 #+ loss_2\n\n    # if k%40==0: print(loss.item())\n    \n    #backward pass\n    W.grad = None #More efficient than setting to zero directly. Lack of gradient is interpreted as zero by PyTorch\n    loss.backward()\n    \n    #update\n    W.data += -4*50 * W.grad\nprint(loss_1.item(), loss_2.item(), loss.item())\n\n2.2389516830444336 0.006938230711966753 2.2389516830444336\n\n\nFinally let’s evaluate the loss on the test set\n\nxs, ys = Xte, Yte\nlogits = W[xs] #log-counts\ncounts = logits.exp() # exponentiate the logits to get fake counts\nprobs = counts/counts.sum(1,keepdims=True)\nloss = (-(probs[torch.arange(xs.nelement()),ys]).log()).mean()\nprint(loss.item())\n\n2.323352575302124",
    "crumbs": [
      "L01_E04"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "makemore",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "makemore"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "makemore",
    "section": "Install",
    "text": "Install\npip install makemore",
    "crumbs": [
      "makemore"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "makemore",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "makemore"
    ]
  },
  {
    "objectID": "backprop.html",
    "href": "backprop.html",
    "title": "Makemore Part 4",
    "section": "",
    "text": "Before we go on to RNN, which are universal approximators, we would like to have a good intuitive understanding of the activations of a neural net during training and especially the gradients that are flowing backwards, how they behave and what they look like. This is required to understand why RNNs are not easily optimizable with the first order gradient based techniques that we use all the time.\n\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n\n\nwords = open('../names.txt','r').read().splitlines()\nwords[:8]\n\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n\n\n\nlen(words)\n\n32033\n\n\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0 #add special dot character to the vocabulary\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n\n\n\n# build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n    X, Y = [], []\n\n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix] # crop and append\n    \n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\n\n\nThe line of code AK takes issue with is the loss.backward(). While in Micrograd we did the backward pass at the level of scalars next we will work to write our backward pass manually on the level of tensors.\nStop: 1m07s/1h55m23s\n\n# utility function we will use later when comparing manual gradients to PyTorch gradients\ndef cmp(s, dt, t):\n    ex = torch.all(dt == t.grad).item()\n    app = torch.allclose(dt, t.grad)\n    maxdiff = (dt - t.grad).abs().max().item()\n    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 64 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\n# Note: I am initializating many of these parameters in non-standard ways\n# because sometimes initializating with e.g. all zeros could mask an incorrect\n# implementation of the backward pass.\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n4137\n\n\n\nbatch_size = 32\nn = batch_size # a shorter variable also, for convenience\n# construct a minibatch\nix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\nXb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n\n# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n\nemb = C[Xb] # embed the characters into vectors\nembcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n# Linear layer 1\nhprebn = embcat @ W1 + b1 # hidden layer pre-activation\n# BatchNorm layer\nbnmeani = 1/n*hprebn.sum(0, keepdim=True)\nbndiff = hprebn - bnmeani\nbndiff2 = bndiff**2\nbnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\nbnvar_inv = (bnvar + 1e-5)**-0.5\nbnraw = bndiff * bnvar_inv\nhpreact = bngain * bnraw + bnbias\n# Non-linearity\nh = torch.tanh(hpreact) # hidden layer\n# Linear layer 2\nlogits = h @ W2 + b2 # output layer\n# cross entropy loss (same as F.cross_entropy(logits, Yb))\nlogit_maxes = logits.max(1, keepdim=True).values\nnorm_logits = logits - logit_maxes # subtract max for numerical stability\ncounts = norm_logits.exp()\ncounts_sum = counts.sum(1, keepdims=True)\ncounts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\nprobs = counts * counts_sum_inv\nlogprobs = probs.log()\nloss = -logprobs[range(n), Yb].mean()\n\n# PyTorch backward pass\nfor p in parameters:\n    p.grad = None\nfor t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n         embcat, emb]:\n    t.retain_grad()\nloss.backward()\nloss\n\ntensor(3.3420, grad_fn=&lt;NegBackward0&gt;)\n\n\n\n# Stop: 12m55s/1h55m23s\n\n\nThe idea here is to ALWAYS come up with a small example of 2 by 2 matrices that you can hand compute the derivatives. This allows us to see the pattern and then put that into the code.\nBessel’s correction is important when we have small batch sizes. There is a bug in Pytorch when we are using BatchNorm1D for small batch sizes because there isn’t a way to tell PyTorch to use the correction.\nObserve the duality when a broadcast happens in the forward pass and then what happens in the backward pass (it turns into a sum) and vice versa.\n\n\n#KS1 - there is broadcasting happening here \n# c = a * b but with tensors\n#a[3x3] * b[3x1] ----&gt;\n# a11*b1 a12*b1 a13*b1\n# a21*b2 a22*b2 a23*b2\n# a31*b3 a32*b3 a33*b3\n# c[3x3]\n\n\n# a11 a12 a13 --&gt; b1 (=a11+a12+a13)\n# a21 a22 a23 --&gt; b2 (=a21+a22+a23)\n# a31 a32 a33 --&gt; b3 (=a31+a32+a33)\n\n\n# dembcat.shape\n\n\n# emb = C[Xb]\n# print(emb.shape, C.shape, Xb.shape)\n# print(Xb[:5])\n\n\n# Exercise 1: backprop through the whole thing manually, \n# backpropagating through exactly all of the variables \n# as they are defined in the forward pass above, one by one\n\n#------------------\n# YOUR CODE HERE :)\n#------------------\ndlogprobs = torch.zeros_like(logprobs) #dlogprobs is shorthand for dL/dlogprobs (here L=loss)\ndlogprobs[range(n), Yb] = -1./n\ndprobs = (1./probs)*dlogprobs #dL/dprobs = (dlogprobs/dprobs) * dL/dlogprobs\ndcounts_sum_inv = (counts * dprobs).sum(dim=1,keepdim=True) #KS1 dL/dcounts_sum_inv = (dprobs/dcounts_sum_inv)*dL/dprobs\n\n# KS2: can't compare to dcounts yet since there is another branch\ndcounts = (counts_sum_inv * dprobs) #KS2 dL/counts = (dprobs/dcounts)*dL/dprobs\n\ndcounts_sum = (-counts_sum**-2) * dcounts_sum_inv #dL/counts_sum = (dcounts_sum_inv/dcounts_sum)*dL/dcounts_sum_inv\ndcounts += torch.ones_like(counts)*dcounts_sum #dL/counts += (dcounts_sum/dcounts)*dL/dcounts_sum\ndnorm_logits = counts * dcounts #dL/dnorm_logits = (dcounts/dnorm_logits)*dL/dcounts\n\n# norm_logits = logits - logit_maxes # subtract max for numerical stability\n# KS 3: broadcasting happens during the subtraction\ndlogit_maxes = (-torch.ones_like(logit_maxes) * dnorm_logits).sum(dim=1,keepdim=True) #KS3 dL/dlogit_maxes = (dnorm_logits/dlogit_maxes) *(dL/dnorm_logits)\n# dlogits = torch.ones_like(logits) * dnorm_logits #dL/dlogits = (dnorm_logits/dlogits) *(dL/dnorm_logits)\ndlogits = dnorm_logits.clone()\n\n# logit_maxes = logits.max(1, keepdim=True).values\n# only allow the gradient to flow towards that logit column which was the maximum on any given example\n# tmp = torch.zeros_like(logits)\n# tmp[range(n),logits.argmax(dim=1)] = 1\n# dlogits += tmp * dlogit_maxes   #dL/dlogits += dlogit_maxes/dlogits * dL/dlogit_maxes\ndlogits += F.one_hot(logits.max(dim=1).indices, num_classes =  logits.shape[1]) * dlogit_maxes\n\n# logits = h @ W2 + b2 # output layer\n# Note the pattern of there being one term for every item\n# in the expression above. And we have the use of the dlogits (global backward gradient) \n# multiplying on the right hand sides\ndh =  dlogits @ W2.T # dL/dh = dlogits/dh * dL/dlogits\ndW2 = h.T @ dlogits\ndb2  = dlogits.sum(dim=0)\n\n# h = torch.tanh(hpreact)\ndhpreact = (1.0 - h**2)* dh# dL/dhpreact = dh/dhpreact * dL/dh\n\n# hpreact = bngain * bnraw + bnbias (ELEMENT WISE multiply with broadcasting here!!)\ndbngain = (bnraw * dhpreact).sum(dim=0,keepdim=True) # dhpreact/bngain * dL/dhpreact\ndbnraw = bngain * dhpreact\ndbnbias = dhpreact.sum(dim=0,keepdim=True)\n\n#bnraw = bndiff * bnvar_inv\ndbnvar_inv = (bndiff * dbnraw).sum(dim=0,keepdim=True)\ndbndiff = bnvar_inv * dbnraw\n\n#bnvar_inv = (bnvar + 1e-5)**-0.5\ndbnvar = (-0.5*((bnvar + 1e-5)**(-1.5)) * dbnvar_inv).sum(dim=0,keepdim=True)\n\n#bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\ndbndiff2 = ( (1/(n-1)) * torch.ones_like(bndiff2) ) *  dbnvar\n\n#bndiff2 = bndiff**2\ndbndiff += (2 * bndiff) * dbndiff2\n\n# bndiff = hprebn - bnmeani\ndbnmeani = (-1.*torch.ones_like(bnmeani) * dbndiff).sum(dim=0,keepdim=True)\ndhprebn = dbndiff.clone()\n\n#bnmeani = 1/n*hprebn.sum(0, keepdim=True)\ndhprebn += ( (1/n) * torch.ones_like(hprebn) ) * dbnmeani\n\n#hprebn = embcat @ W1 + b1\ndembcat = dhprebn @ W1.T\ndW1 = embcat.T @ dhprebn\ndb1  = dhprebn.sum(dim=0)\n\n#embcat = emb.view(emb.shape[0], -1)\ndemb = dembcat.view(emb.shape)\n\n# emb = C[Xb]\ndC = torch.zeros_like(C)\nfor k in range(Xb.shape[0]): #iterate over each example\n    for j in range(Xb.shape[1]): #over each character in the context\n        ix = Xb[k,j] # look up the character's index\n        dC[ix] += demb[k,j] # deposit the gradient (multiple times if necessary) #1h25m50s\n\ncmp('logprobs', dlogprobs, logprobs)\ncmp('probs', dprobs, probs)\ncmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv) #26m20s\ncmp('counts_sum', dcounts_sum, counts_sum) #28m54s\ncmp('counts', dcounts, counts) #32m28s\ncmp('norm_logits', dnorm_logits, norm_logits) #33m10s\ncmp('logit_maxes', dlogit_maxes, logit_maxes)\ncmp('logits', dlogits, logits)#41m47s\ncmp('h', dh, h)\ncmp('W2', dW2, W2)\ncmp('b2', db2, b2)\ncmp('hpreact', dhpreact, hpreact)\ncmp('bngain', dbngain, bngain)\ncmp('bnraw', dbnraw, bnraw)\ncmp('bnbias', dbnbias, bnbias)\ncmp('bnvar_inv', dbnvar_inv, bnvar_inv)\ncmp('bnvar', dbnvar, bnvar)\ncmp('bndiff2', dbndiff2, bndiff2)\ncmp('bndiff', dbndiff, bndiff)\ncmp('bnmeani', dbnmeani, bnmeani)\ncmp('hprebn', dhprebn, hprebn)\ncmp('embcat', dembcat, embcat)\ncmp('W1', dW1, W1)\ncmp('b1', db1, b1)\ncmp('emb', demb, emb)\ncmp('C', dC, C)\n\nlogprobs        | exact: True  | approximate: True  | maxdiff: 0.0\nprobs           | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\ncounts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\ncounts          | exact: True  | approximate: True  | maxdiff: 0.0\nnorm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\nlogit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\nlogits          | exact: True  | approximate: True  | maxdiff: 0.0\nh               | exact: True  | approximate: True  | maxdiff: 0.0\nW2              | exact: True  | approximate: True  | maxdiff: 0.0\nb2              | exact: True  | approximate: True  | maxdiff: 0.0\nhpreact         | exact: True  | approximate: True  | maxdiff: 0.0\nbngain          | exact: True  | approximate: True  | maxdiff: 0.0\nbnraw           | exact: True  | approximate: True  | maxdiff: 0.0\nbnbias          | exact: True  | approximate: True  | maxdiff: 0.0\nbnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\nbnvar           | exact: True  | approximate: True  | maxdiff: 0.0\nbndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\nbndiff          | exact: True  | approximate: True  | maxdiff: 0.0\nbnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\nhprebn          | exact: True  | approximate: True  | maxdiff: 0.0\nembcat          | exact: True  | approximate: True  | maxdiff: 0.0\nW1              | exact: True  | approximate: True  | maxdiff: 0.0\nb1              | exact: True  | approximate: True  | maxdiff: 0.0\nemb             | exact: True  | approximate: True  | maxdiff: 0.0\nC               | exact: True  | approximate: True  | maxdiff: 0.0\n\n\n\n# 04/07/2023 - Stop: 20m55s/1h55m23s\n# 04/08/2023 - Stop: 33m10s/1h55m23s\n# 04/09/2023 - Stop: 55m14s/1h55m23s\n# 04/10/2023 - Stop: 59m41s/1h55m23s\n# 04/11/2023 - Stop: 1h26m27s/1h55m23s (backpropogated through the entire beast!!!)\n\n\n# Exercise 2: backprop through cross_entropy but all in one go\n# to complete this challenge look at the mathematical expression of the loss,\n# take the derivative, simplify the expression, and just write it out\n\n# forward pass\n\n# before:\n# logit_maxes = logits.max(1, keepdim=True).values\n# norm_logits = logits - logit_maxes # subtract max for numerical stability\n# counts = norm_logits.exp()\n# counts_sum = counts.sum(1, keepdims=True)\n# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n# probs = counts * counts_sum_inv\n# logprobs = probs.log()\n# loss = -logprobs[range(n), Yb].mean()\n\n# now:\nloss_fast = F.cross_entropy(logits, Yb)\nprint(loss_fast.item(), 'diff:', (loss_fast - loss).item())\n\n3.341958999633789 diff: 0.0\n\n\n\nlogits.shape, Yb.shape, (F.one_hot(Yb, num_classes =  logits.shape[1])).shape\n\n(torch.Size([32, 27]), torch.Size([32]), torch.Size([32, 27]))\n\n\n\n# backward pass\n#------------------\n# YOUR CODE HERE :)\n#------------------\n# https://nasheqlbrm.github.io/blog/posts/2021-08-07-cross-entropy-loss-pytorch.html\n# dlogits = (1./n) * ( probs - F.one_hot(Yb, num_classes =  probs.shape[1]) )\ndlogits = F.softmax(logits, 1)\ndlogits[range(n), Yb] -= 1\ndlogits /= n\ncmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9\n\nlogits          | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n\n\n\nlogits.shape, Yb.shape\n\n(torch.Size([32, 27]), torch.Size([32]))\n\n\n\nF.softmax(logits, 1)[0]\n\ntensor([0.0712, 0.0899, 0.0199, 0.0502, 0.0185, 0.0781, 0.0247, 0.0341, 0.0174,\n        0.0312, 0.0381, 0.0373, 0.0371, 0.0296, 0.0367, 0.0142, 0.0096, 0.0187,\n        0.0161, 0.0524, 0.0510, 0.0210, 0.0235, 0.0757, 0.0571, 0.0249, 0.0218],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\n# same as the previous cell except for the location of the label for the example (index (zero based) \n# 8 in this case)\ndlogits[0] * n\n\ntensor([ 0.0712,  0.0899,  0.0199,  0.0502,  0.0185,  0.0781,  0.0247,  0.0341,\n        -0.9826,  0.0312,  0.0381,  0.0373,  0.0371,  0.0296,  0.0367,  0.0142,\n         0.0096,  0.0187,  0.0161,  0.0524,  0.0510,  0.0210,  0.0235,  0.0757,\n         0.0571,  0.0249,  0.0218], grad_fn=&lt;MulBackward0&gt;)\n\n\n\n# dlogits is the amount of push and pull exerted on the probabilities\n# we pull up the probability of the correct example and push down on\n# the others in a way such that the sum of the pushes and pull sums to zero\ndlogits[0].sum()\n\ntensor(4.1910e-09, grad_fn=&lt;SumBackward0&gt;)\n\n\n\n# Here the dark corresponds to a pull\n# the lights correspond to the pushes.\n# Thus, training a neural network can be thought of as some physical pulley\n# system where these pushes and pull are gradually shaping the weights and biases\nplt.figure(figsize=(4, 4))\nplt.imshow(dlogits.detach(), cmap='gray')\n\n\n\n\n\n\n\n\n\n# 04/12/2023 - Stop: 1h36m39s/1h55m23s\n\n\n# Exercise 3: backprop through batchnorm but all in one go\n# to complete this challenge look at the mathematical expression of the output of batchnorm,\n# take the derivative w.r.t. its input, simplify the expression, and just write it out\n\n# forward pass\n\n# before:\n# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n# bndiff = hprebn - bnmeani\n# bndiff2 = bndiff**2\n# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n# bnvar_inv = (bnvar + 1e-5)**-0.5\n# bnraw = bndiff * bnvar_inv\n# hpreact = bngain * bnraw + bnbias\n\n# now:\nhpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\nprint('max diff:', (hpreact_fast - hpreact).abs().max())\n\nmax diff: tensor(4.7684e-07, grad_fn=&lt;MaxBackward1&gt;)\n\n\n \n\n# backward pass\n\n# before we had:\n# dbnraw = bngain * dhpreact\n# dbndiff = bnvar_inv * dbnraw\n# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n# dbndiff += (2*bndiff) * dbndiff2\n# dhprebn = dbndiff.clone()\n# dbnmeani = (-dbndiff).sum(0)\n# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n\n# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n# (you'll also need to use some of the variables from the forward pass up above)\n\n# the summation over the j-s is over the training examples\ndhprebn = bngain * bnvar_inv/n *( n*dhpreact - dhpreact.sum(0,keepdim=True) - \n                                 (n/(n-1)) * bnraw * ( dhpreact * bnraw ).sum(0,keepdim=True) )\n\ncmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10\n\nhprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n\n\n\ndhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape\n\n(torch.Size([32, 64]),\n torch.Size([1, 64]),\n torch.Size([1, 64]),\n torch.Size([32, 64]),\n torch.Size([64]))\n\n\n\n# 04/13/2023 - Stop: 1h46m03s/1h55m23s\n# 04/13/2023 - Stop: 1h50m08s/1h55m23s\n\n\n# Exercise 4: putting it all together!\n# Train the MLP neural net with your own backward pass\n\n# init\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\ng = torch.Generator().manual_seed(2147483647) # for reproducibility\nC  = torch.randn((vocab_size, n_embd),            generator=g)\n# Layer 1\nW1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\nb1 = torch.randn(n_hidden,                        generator=g) * 0.1\n# Layer 2\nW2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\nb2 = torch.randn(vocab_size,                      generator=g) * 0.1\n# BatchNorm parameters\nbngain = torch.randn((1, n_hidden))*0.1 + 1.0\nbnbias = torch.randn((1, n_hidden))*0.1\n\nparameters = [C, W1, b1, W2, b2, bngain, bnbias]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nn = batch_size # convenience\nlossi = []\n\n# use this context manager for efficiency once your backward pass is written (TODO)\nwith torch.no_grad():\n\n  # kick off optimization\n  for i in range(max_steps):\n\n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n    # Linear layer\n    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n    # BatchNorm layer\n    # -------------------------------------------------------------\n    bnmean = hprebn.mean(0, keepdim=True)\n    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n    bnvar_inv = (bnvar + 1e-5)**-0.5\n    bnraw = (hprebn - bnmean) * bnvar_inv\n    hpreact = bngain * bnraw + bnbias\n    # -------------------------------------------------------------\n    # Non-linearity\n    h = torch.tanh(hpreact) # hidden layer\n    logits = h @ W2 + b2 # output layer\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    #loss.backward() # use this for correctness comparisons, delete it later!\n\n    # manual backprop! #swole_doge_meme\n    # -----------------\n    dlogits = F.softmax(logits, 1)\n    dlogits[range(n), Yb] -= 1\n    dlogits /= n\n    # 2nd layer backprop\n    dh = dlogits @ W2.T\n    dW2 = h.T @ dlogits\n    db2 = dlogits.sum(0)\n    # tanh\n    dhpreact = (1.0 - h**2) * dh\n    # batchnorm backprop\n    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n    dbnbias = dhpreact.sum(0, keepdim=True)\n    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n    # 1st layer\n    dembcat = dhprebn @ W1.T\n    dW1 = embcat.T @ dhprebn\n    db1 = dhprebn.sum(0)\n    # embedding\n    demb = dembcat.view(emb.shape)\n    dC = torch.zeros_like(C)\n    for k in range(Xb.shape[0]):\n        for j in range(Xb.shape[1]):\n            ix = Xb[k,j]\n            dC[ix] += demb[k,j]\n    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n    # -----------------\n\n    # update\n    lr = 0.1 if i &lt; 100000 else 0.01 # step learning rate decay\n    for p, grad in zip(parameters, grads):\n      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n      p.data += -lr * grad # new way of swole doge TODO: enable\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n\n  #   if i &gt;= 100: # TODO: delete early breaking when you're ready to train the full net\n  #     break\n\n12297\n      0/ 200000: 3.8114\n  10000/ 200000: 2.1689\n  20000/ 200000: 2.4194\n  30000/ 200000: 2.4330\n  40000/ 200000: 1.9810\n  50000/ 200000: 2.3530\n  60000/ 200000: 2.3664\n  70000/ 200000: 2.0867\n  80000/ 200000: 2.3298\n  90000/ 200000: 2.0542\n 100000/ 200000: 1.9986\n 110000/ 200000: 2.3207\n 120000/ 200000: 1.9800\n 130000/ 200000: 2.4701\n 140000/ 200000: 2.3593\n 150000/ 200000: 2.2122\n 160000/ 200000: 1.9384\n 170000/ 200000: 1.8528\n 180000/ 200000: 1.9438\n 190000/ 200000: 1.8624\n\n\n\n# useful for checking your gradients\n# This will require you to \n# 1. take out the with torch.no_grad()\n# 2. uncomment the loss.backward()\n# 3. uncomment the first line and comment the second line for where we do the gradient update \n# 4. and finally the portion at the end of the previous cell \n# where we only want to do a 100 iterations\n# for p,g in zip(parameters, grads):\n#   cmp(str(tuple(p.shape)), g, p)\n\n\n# calibrate the batch norm at the end of training\n# because we didn't keep track of the running mean and variance\n# in the training loop\nwith torch.no_grad():\n    # pass the training set through\n    emb = C[Xtr]\n    embcat = emb.view(emb.shape[0], -1)\n    hpreact = embcat @ W1 + b1\n    # measure the mean/std over the entire training set\n    bnmean = hpreact.mean(0, keepdim=True)\n    bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n\n\n# evaluate train and val loss\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    emb = C[x] # (N, block_size, n_embd)\n    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n    hpreact = embcat @ W1 + b1\n    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n    h = torch.tanh(hpreact) # (N, n_hidden)\n    logits = h @ W2 + b2 # (N, vocab_size)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 2.0728228092193604\nval 2.1120364665985107\n\n\n\n# sample from the model\ng = torch.Generator().manual_seed(2147483647 + 10)\n\nfor _ in range(20):\n    \n    out = []\n    context = [0] * block_size # initialize with all ...\n    while True:\n        # ------------\n        # forward pass:\n        # Embedding\n        emb = C[torch.tensor([context])] # (1,block_size,d)      \n        embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n        hpreact = embcat @ W1 + b1\n        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n        h = torch.tanh(hpreact) # (N, n_hidden)\n        logits = h @ W2 + b2 # (N, vocab_size)\n        # ------------\n        # Sample\n        probs = F.softmax(logits, dim=1)\n        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n        context = context[1:] + [ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    \n    print(''.join(itos[i] for i in out))\n\ncarmah.\namori.\nkifirmili.\ntaty.\nskanden.\njazhitlangelynn.\nkaeli.\nnellara.\nchaiiv.\nkaleigh.\nham.\njoce.\nquinn.\nsalin.\nalianni.\nwaterridearisi.\njaxenni.\nsabee.\ndeci.\nabette.\n\n\n\n# 04/13/2023 - Fin: 1h55m23s/1h55m23s",
    "crumbs": [
      "Makemore Part 4"
    ]
  },
  {
    "objectID": "l01_e05.html",
    "href": "l01_e05.html",
    "title": "L01_E05",
    "section": "",
    "text": "Look up and use F.cross_entropy instead. You should achieve the same result. Can you think of why we’d prefer to use F.cross_entropy instead?\n\nimport torch\n\n\nimport random\nrandom.seed(42)\n\nwords = open('../names.txt','r').read().splitlines()\n\nrandom.shuffle(words)\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\n\nwords_tr = words[:n1]\nwords_dev = words[n1:n2]\nwords_te = words[n2:]\n\n\nchars = sorted(list(set(''.join(words_tr))))\nlen(chars)\n\n26\n\n\n\nctoi = {c : i+1 for i,c in enumerate(chars)}\nctoi['.'] = 0\n\n\nitoc = {i:c for c,i in ctoi.items()}\n\n\nnum_chars = len(ctoi.keys())\nnum_chars\n\n27\n\n\n\nstoi = {}\nfor i0,c0 in sorted(itoc.items(), key=lambda kv: kv[0]):\n    for i1,c1 in sorted(itoc.items(), key=lambda kv: kv[0]):\n        #print((i0*num_chars) + i1,c0,c1)\n        stoi[(c0,c1)] = (i0*num_chars) + i1\n\n\ndef build_dataset(words):\n    xs,ys = [],[]\n    \n    for word in words:\n        chs = '..' + word + '.'\n        for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n            ix1 = stoi[ch1,ch2]\n            ix2 = ctoi[ch3]\n            xs.append(ix1)\n            ys.append(ix2)\n    \n    # prefer to use torch.tensor instead of torch.Tensor\n    xs = torch.tensor(xs)\n    ys = torch.tensor(ys)\n    num = xs.nelement()\n    print(f'number of examples: {num}')   \n\n    return xs, ys\n\n\nXtr,Ytr=build_dataset(words_tr)\nXdev,Ydev=build_dataset(words_dev)\nXte,Yte=build_dataset(words_te)\n\nnumber of examples: 182625\nnumber of examples: 22655\nnumber of examples: 22866\n\n\n\nXtr.dtype\n\ntorch.int64\n\n\n\nimport torch.nn.functional as F\n\n\ng = torch.Generator().manual_seed(2147483647)\nW = torch.randn((num_chars*num_chars,num_chars), generator=g, requires_grad=True) #single layer of 27 neurons each getting 27x27 inputs\n\nreg = 0.004552933843974289\nfor k in range(400):\n    xs, ys = Xtr, Ytr\n    logits = W[xs] #log-counts\n    \n    loss_1 = F.cross_entropy(logits, ys)\n    loss_2 = reg*(W**2).mean() #regularization loss\n    \n    loss = loss_1 #+ loss_2\n\n    # if k%40==0: print(loss.item())\n    \n    #backward pass\n    W.grad = None #More efficient than setting to zero directly. Lack of gradient is interpreted as zero by PyTorch\n    loss.backward()\n    \n    #update\n    W.data += -4*50 * W.grad\nprint(loss_1.item(), loss_2.item(), loss.item())\n\n2.2274816036224365 0.006936684716492891 2.2274816036224365\n\n\nFinally let’s evaluate the loss on the test set\n\nxs, ys = Xte, Yte\nlogits = W[xs] #log-counts\nloss = F.cross_entropy(logits, ys)\nprint(loss.item())\n\n2.268364906311035",
    "crumbs": [
      "L01_E05"
    ]
  },
  {
    "objectID": "cnn1.html",
    "href": "cnn1.html",
    "title": "Makemore Part 5",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt # for making figures\n# read in all the words\nwords = open('../names.txt', 'r').read().splitlines()\nprint(len(words))\nprint(max(len(w) for w in words))\nprint(words[:8])\n\n32033\n15\n['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\nvocab_size = len(itos)\nprint(itos)\nprint(vocab_size)\n\n{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n27\n# shuffle up the words\nimport random\nrandom.seed(42)\nrandom.shuffle(words)\n# build the dataset\nblock_size = 3 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n    X, Y = [], []\n  \n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix] # crop and append\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n\ntorch.Size([182625, 3]) torch.Size([182625])\ntorch.Size([22655, 3]) torch.Size([22655])\ntorch.Size([22866, 3]) torch.Size([22866])\nfor x,y in zip(Xtr[:20], Ytr[:20]):\n    print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()])\n\n... --&gt; y\n..y --&gt; u\n.yu --&gt; h\nyuh --&gt; e\nuhe --&gt; n\nhen --&gt; g\neng --&gt; .\n... --&gt; d\n..d --&gt; i\n.di --&gt; o\ndio --&gt; n\nion --&gt; d\nond --&gt; r\nndr --&gt; e\ndre --&gt; .\n... --&gt; x\n..x --&gt; a\n.xa --&gt; v\nxav --&gt; i\navi --&gt; e\n# Near copy paste of the layers we have developed in Part 3\n\n# -----------------------------------------------------------------------------------------------\nclass Linear:\n  \n    def __init__(self, fan_in, fan_out, bias=True):\n        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note: kaiming init\n        self.bias = torch.zeros(fan_out) if bias else None\n  \n    def __call__(self, x):\n        self.out = x @ self.weight\n        if self.bias is not None:\n            self.out += self.bias\n        return self.out\n  \n    def parameters(self):\n        return [self.weight] + ([] if self.bias is None else [self.bias])\n# -----------------------------------------------------------------------------------------------\nclass BatchNorm1d:\n  \n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running 'momentum update')\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n  \n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            xmean = x.mean(0, keepdim=True) # batch mean\n            xvar = x.var(0, keepdim=True) # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        return self.out\n  \n    def parameters(self):\n        return [self.gamma, self.beta]\n# -----------------------------------------------------------------------------------------------\nclass Tanh:\n    def __call__(self, x):\n        self.out = torch.tanh(x)\n        return self.out\n    def parameters(self):\n        return []\ntorch.manual_seed(42); # seed rng for reproducibility\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\nC = torch.randn((vocab_size, n_embd)) # embedding table\nlayers = [Linear(n_embd*block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n          Linear(n_hidden, vocab_size)\n         ]\n\n# parameter init\nwith torch.no_grad():\n    layers[-1].weight *= 0.1 # last layer make less confident at initialization\n\nparameters = [C] + [p for layer in layers for p in layer.parameters()]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n12097\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    emb = C[Xb] # embed the characters into vectors\n    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n#     break\n\n      0/ 200000: 3.2966\n  10000/ 200000: 2.2322\n  20000/ 200000: 2.4111\n  30000/ 200000: 2.1004\n  40000/ 200000: 2.3157\n  50000/ 200000: 2.2104\n  60000/ 200000: 1.9653\n  70000/ 200000: 1.9767\n  80000/ 200000: 2.6738\n  90000/ 200000: 2.0837\n 100000/ 200000: 2.2730\n 110000/ 200000: 1.7491\n 120000/ 200000: 2.2891\n 130000/ 200000: 2.3443\n 140000/ 200000: 2.1731\n 150000/ 200000: 1.8246\n 160000/ 200000: 1.7614\n 170000/ 200000: 2.2419\n 180000/ 200000: 2.0803\n 190000/ 200000: 2.1326\nplt.plot(lossi)\nlossi[:10]\n\n[0.5180676579475403,\n 0.5164594054222107,\n 0.507362961769104,\n 0.507546603679657,\n 0.4992470443248749,\n 0.5014019012451172,\n 0.5049523115158081,\n 0.48866209387779236,\n 0.4999050199985504,\n 0.4899313449859619]\nThe above plot was a “dagger in the eyes”. There is a lot of variability in loss from one batch to the next (because of our relatively small batch size of 32 training examples). Let’s do a plot where we visualize the loss like so:\ntorch.tensor(lossi).view(-1,1000).shape\n\ntorch.Size([200, 1000])\nplt.plot(torch.tensor(lossi).view(-1,1000).mean(1))\n# put layers into eval mode (needed for batchnorm especially)\nfor layer in layers:\n    layer.training = False\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    emb = C[x] # (N, block_size, n_embd)\n    x = emb.view(emb.shape[0], -1) # concatenate the vectors\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 2.0583250522613525\nval 2.1065289974212646\nfor _ in range(20):\n    out=[]\n    context = [0]*block_size\n    while True:\n        emb = C[torch.tensor([context])]\n        x = emb.view(emb.shape[0], -1) # concatenate the vectors\n        for layer in layers:\n            x = layer(x)\n        logits = x\n        probs = F.softmax(logits, dim=1)\n        \n        ix = torch.multinomial(probs, num_samples=1).item()\n        context = context[1:]+[ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    print(''.join([itos[i] for i in out]))\n\nivon.\nfanili.\nthoommara.\nkelo.\nmatyn.\nleandr.\naleigh.\nkoldeniah.\nprus.\ncarleen.\njah.\njorra.\nalaya.\nshonan.\nvishylaharia.\njuna.\nvio.\norven.\nmina.\nlaylee.",
    "crumbs": [
      "Makemore Part 5"
    ]
  },
  {
    "objectID": "cnn1.html#introduce-embedding-and-flatten",
    "href": "cnn1.html#introduce-embedding-and-flatten",
    "title": "Makemore Part 5",
    "section": "Introduce Embedding and Flatten",
    "text": "Introduce Embedding and Flatten\n\n# -----------------------------------------------------------------------------------------------\nclass Embedding:\n  \n    def __init__(self, num_embeddings, embedding_dim):\n        self.weight = torch.randn((num_embeddings, embedding_dim))\n    \n    def __call__(self, IX):\n        self.out = self.weight[IX]\n        return self.out\n  \n    def parameters(self):\n        return [self.weight]\n\n# -----------------------------------------------------------------------------------------------\nclass Flatten:\n    \n    def __call__(self,x):\n        self.out = x.view(x.shape[0],-1)\n        return self.out\n    \n    def parameters(self):\n        return []\n\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nlayers = [Embedding(vocab_size, n_embd),\n          Flatten(),\n          Linear(n_embd*block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n          Linear(n_hidden, vocab_size)\n         ]\n\n# parameter init\nwith torch.no_grad():\n    layers[-1].weight *= 0.1 # last layer make less confident at initialization\n\nparameters = [p for layer in layers for p in layer.parameters()]\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n12097\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    x = Xb\n    for layer in layers:\n        x = layer(x)\n    loss = F.cross_entropy(x, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n    break\n\n      0/ 200000: 3.2877",
    "crumbs": [
      "Makemore Part 5"
    ]
  },
  {
    "objectID": "cnn1.html#introduce-sequential",
    "href": "cnn1.html#introduce-sequential",
    "title": "Makemore Part 5",
    "section": "Introduce Sequential",
    "text": "Introduce Sequential\n\n# -----------------------------------------------------------------------------------------------\nclass Sequential:\n  \n    def __init__(self, layers):\n        self.layers = layers\n  \n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n        self.out = x\n        return self.out\n  \n    def parameters(self):\n        # get parameters of all layers and stretch them out into one list\n        return [p for layer in self.layers for p in layer.parameters()]\n\n\n# Stop: 04/14/2023 13m22s/56m21s\n\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nmodel = Sequential([Embedding(vocab_size, n_embd),\n                    Flatten(),\n                    Linear(n_embd*block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n                    Linear(n_hidden, vocab_size)\n                    ]\n                   )\n\n# parameter init\nwith torch.no_grad():\n    model.layers[-1].weight *= 0.1 # last layer make less confident at initialization\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n12097\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    logits = model(Xb)\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n#     break\n\n      0/ 200000: 3.2951\n  10000/ 200000: 2.4608\n  20000/ 200000: 1.9612\n  30000/ 200000: 2.2665\n  40000/ 200000: 2.0159\n  50000/ 200000: 2.6640\n  60000/ 200000: 2.0771\n  70000/ 200000: 2.2932\n  80000/ 200000: 2.4355\n  90000/ 200000: 2.3301\n 100000/ 200000: 2.2692\n 110000/ 200000: 2.2957\n 120000/ 200000: 2.3526\n 130000/ 200000: 2.0627\n 140000/ 200000: 2.5461\n 150000/ 200000: 1.8184\n 160000/ 200000: 2.1374\n 170000/ 200000: 2.3346\n 180000/ 200000: 2.0952\n 190000/ 200000: 1.6799\n\n\n\nplt.plot(torch.tensor(lossi).view(-1,1000).mean(1))\n\n\n\n\n\n\n\n\n\n# put layers into eval mode (needed for batchnorm especially)\nfor layer in model.layers:\n    layer.training = False\n\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    logits = model(x)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 2.0593814849853516\nval 2.104369640350342\n\n\n\nfor _ in range(20):\n    out=[]\n    context = [0]*block_size\n    while True:\n        logits = model(torch.tensor([context]))\n        probs = F.softmax(logits, dim=1)\n        \n        ix = torch.multinomial(probs, num_samples=1).item()\n        context = context[1:]+[ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    print(''.join([itos[i] for i in out]))\n\nforalrabhithaan.\nman.\nreema.\nana.\ndeanyell.\ntedaryia.\nsaove.\nadise.\nmadie.\nkasuka.\nlakileanni.\nshaadora.\nzhan.\nevre.\nrae.\nstevin.\nel.\nkamelian.\nkamelynny.\ndalucki.",
    "crumbs": [
      "Makemore Part 5"
    ]
  },
  {
    "objectID": "cnn1.html#naive-scale-up",
    "href": "cnn1.html#naive-scale-up",
    "title": "Makemore Part 5",
    "section": "Naive scale up",
    "text": "Naive scale up\nBy updating the block Size\n\n# build the dataset\nblock_size = 8 # context length: how many characters do we take to predict the next one?\n\ndef build_dataset(words):  \n    X, Y = [], []\n  \n    for w in words:\n        context = [0] * block_size\n        for ch in w + '.':\n            ix = stoi[ch]\n            X.append(context)\n            Y.append(ix)\n            context = context[1:] + [ix] # crop and append\n\n    X = torch.tensor(X)\n    Y = torch.tensor(Y)\n    print(X.shape, Y.shape)\n    return X, Y\n\nn1 = int(0.8*len(words))\nn2 = int(0.9*len(words))\nXtr,  Ytr  = build_dataset(words[:n1])     # 80%\nXdev, Ydev = build_dataset(words[n1:n2])   # 10%\nXte,  Yte  = build_dataset(words[n2:])     # 10%\n\ntorch.Size([182625, 8]) torch.Size([182625])\ntorch.Size([22655, 8]) torch.Size([22655])\ntorch.Size([22866, 8]) torch.Size([22866])\n\n\n\nfor x, y in zip(Xtr[:20],Ytr[:20]):\n    print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()])\n\n........ --&gt; y\n.......y --&gt; u\n......yu --&gt; h\n.....yuh --&gt; e\n....yuhe --&gt; n\n...yuhen --&gt; g\n..yuheng --&gt; .\n........ --&gt; d\n.......d --&gt; i\n......di --&gt; o\n.....dio --&gt; n\n....dion --&gt; d\n...diond --&gt; r\n..diondr --&gt; e\n.diondre --&gt; .\n........ --&gt; x\n.......x --&gt; a\n......xa --&gt; v\n.....xav --&gt; i\n....xavi --&gt; e\n\n\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nmodel = Sequential([Embedding(vocab_size, n_embd),\n                    Flatten(),\n                    Linear(n_embd*block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n                    Linear(n_hidden, vocab_size)\n                    ]\n                   )\n\n# parameter init\nwith torch.no_grad():\n    model.layers[-1].weight *= 0.1 # last layer make less confident at initialization\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n22097\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    logits = model(Xb)\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n#     break\n\n      0/ 200000: 3.2832\n  10000/ 200000: 2.4753\n  20000/ 200000: 2.1789\n  30000/ 200000: 1.8599\n  40000/ 200000: 2.2194\n  50000/ 200000: 2.6839\n  60000/ 200000: 2.1948\n  70000/ 200000: 2.2742\n  80000/ 200000: 2.1448\n  90000/ 200000: 1.8690\n 100000/ 200000: 2.0104\n 110000/ 200000: 1.6630\n 120000/ 200000: 2.0151\n 130000/ 200000: 1.8984\n 140000/ 200000: 1.9967\n 150000/ 200000: 2.0680\n 160000/ 200000: 1.8812\n 170000/ 200000: 2.2087\n 180000/ 200000: 1.5459\n 190000/ 200000: 1.5059\n\n\n\nplt.plot(torch.tensor(lossi).view(-1,1000).mean(1))\n\n\n\n\n\n\n\n\n\n# put layers into eval mode (needed for batchnorm especially)\nfor layer in model.layers:\n    layer.training = False\n\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    logits = model(x)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 1.918139100074768\nval 2.0279526710510254\n\n\n\nfor _ in range(20):\n    out=[]\n    context = [0]*block_size\n    while True:\n        logits = model(torch.tensor([context]))\n        probs = F.softmax(logits, dim=1)\n        \n        ix = torch.multinomial(probs, num_samples=1).item()\n        context = context[1:]+[ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    print(''.join([itos[i] for i in out]))\n\nmacin.\navyren.\nsolamor.\ntaerie.\nbribith.\nsalif.\nmaagraca.\ndaveri.\njaden.\nleyannah.\nkion.\nkatzia.\nrhyon.\nsylver.\ngavena.\ntheona.\nzackir.\nbevelo.\nkiairos.\nkandon.",
    "crumbs": [
      "Makemore Part 5"
    ]
  },
  {
    "objectID": "cnn1.html#scratch-space",
    "href": "cnn1.html#scratch-space",
    "title": "Makemore Part 5",
    "section": "Scratch Space",
    "text": "Scratch Space\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nmodel = Sequential([Embedding(vocab_size, n_embd),\n                    Flatten(),\n                    Linear(n_embd*block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n                    Linear(n_hidden, vocab_size)\n                    ]\n                   )\n\n# parameter init\nwith torch.no_grad():\n    model.layers[-1].weight *= 0.1 # last layer make less confident at initialization\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n22097\n\n\n\nix = torch.randint(0,Xtr.shape[0],(4,)) # Let's look at a batch of just 4 examples\nXb, Yb = Xtr[ix], Ytr[ix]\nlogits = model(Xb) # logits aka logcounts\nprint(Xb.shape)\nXb\n\ntorch.Size([4, 8])\n\n\ntensor([[ 0,  0,  0, 10,  1,  9,  4,  5],\n        [ 0,  0,  0,  0,  0,  0,  0,  3],\n        [ 0,  5, 12, 12,  1, 11,  1, 20],\n        [ 0,  0,  0,  0,  0,  0,  0,  0]])\n\n\n\nmodel.layers[0].out.shape # output of embedding layer\n\ntorch.Size([4, 8, 10])\n\n\n\nmodel.layers[1].out.shape # output of Flatten layer\n\ntorch.Size([4, 80])\n\n\n\nmodel.layers[2].out.shape # output of Linear layer\n\ntorch.Size([4, 200])\n\n\n\n(torch.randn(4,80) @ torch.randn(80,200) + torch.randn(200) ).shape\n\ntorch.Size([4, 200])\n\n\n\nA surprise\nThe torch.randn(4,80) doesn’t have to be a two dimensional. PyTorch’s matrix multiply operation is quite powerful and is able to consume higher dimensional things as well. So, for example,\n\n(torch.randn(4,5,80) @ torch.randn(80,200) + torch.randn(200) ).shape\n\ntorch.Size([4, 5, 200])\n\n\n\n(torch.randn(2,4,80) @ torch.randn(80,200) + torch.randn(200) ).shape\n\ntorch.Size([2, 4, 200])\n\n\nThe dimensions on the left of the 80 are basically treated as batch dimensions. The matrix multiply is happening with the last dimension.\nThis is quite convenient as we can use it for our Wavenet extension where we don’t want to flatten the 8 characters coming in quite at once. We want to create 4 groups of bigrams that we will process in parallel and then build upon them subsequently.\n\n# want this to be 4 groups. Each group with 2 characters (each character becoming a 10 dimensional vector)\n# (1,2),(3,4),(5,6),(7,8)\n\n\nlist(range(10))\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nlist(range(10))[::2] # pull out the even elements from the start\n\n[0, 2, 4, 6, 8]\n\n\n\nlist(range(10))[1::2] # pull out the odd elements starting at 1\n\n[1, 3, 5, 7, 9]\n\n\n\ne = torch.randn(4,8,10) # goal: want this to be (4,4,20) where consecutive 10-d vectors get concatenated \n# We can explicitly pull out the even and odd elements from the context and fuse them\nexplicit = torch.cat([e[:, ::2, :], e[:, 1::2, :]],dim=2)\nexplicit.shape\n\ntorch.Size([4, 4, 20])\n\n\n\n#  we can also just use view\n(e.view(4,4,20) == explicit).all()\n\ntensor(True)\n\n\n\n# 4 examples with 4 groups with each group having a 20 dimensional vectore\ntorch.cat([e[:, ::2, :], e[:, 1::2, :]], dim=2).shape\n\ntorch.Size([4, 4, 20])",
    "crumbs": [
      "Makemore Part 5"
    ]
  },
  {
    "objectID": "cnn1.html#flatten-consecutive",
    "href": "cnn1.html#flatten-consecutive",
    "title": "Makemore Part 5",
    "section": "Flatten consecutive",
    "text": "Flatten consecutive\n\n# -----------------------------------------------------------------------------------------------\nclass FlattenConsecutive:\n  \n    def __init__(self, n):\n        self.n = n\n    \n    def __call__(self, x):\n        B, T, C = x.shape\n        x = x.view(B, T//self.n, C*self.n)\n        if x.shape[1] == 1:\n            x = x.squeeze(1)\n        self.out = x\n        return self.out\n  \n    def parameters(self):\n        return []\n\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 200 # the number of neurons in the hidden layer of the MLP\n\nmodel = Sequential([Embedding(vocab_size, n_embd),\n                    FlattenConsecutive(2),Linear(n_embd*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n                    FlattenConsecutive(2),Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n                    FlattenConsecutive(2),Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),                    \n                    Linear(n_hidden, vocab_size)\n                    ]\n                   )\n\n# parameter init\nwith torch.no_grad():\n    model.layers[-1].weight *= 0.1 # last layer make less confident at initialization\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n170897\n\n\n\nix = torch.randint(0,Xtr.shape[0],(4,)) # Let's look at a batch of just 4 examples\nXb, Yb = Xtr[ix], Ytr[ix]\nlogits = model(Xb) # logits aka logcounts\nprint(Xb.shape)\nXb\n\ntorch.Size([4, 8])\n\n\ntensor([[ 0,  0,  0,  0,  0,  0, 11,  1],\n        [ 0,  0,  0,  0,  8,  1, 14, 14],\n        [ 0,  1,  5, 18,  9, 12, 25, 14],\n        [ 0,  0,  0,  1, 26,  9, 26,  2]])\n\n\n\nfor layer in model.layers:\n    print(layer.__class__.__name__,':',tuple(layer.out.shape))\n\nEmbedding : (4, 8, 10)\nFlattenConsecutive : (4, 4, 20)\nLinear : (4, 4, 200)\nBatchNorm1d : (4, 4, 200)\nTanh : (4, 4, 200)\nFlattenConsecutive : (4, 2, 400)\nLinear : (4, 2, 200)\nBatchNorm1d : (4, 2, 200)\nTanh : (4, 2, 200)\nFlattenConsecutive : (4, 400)\nLinear : (4, 200)\nBatchNorm1d : (4, 200)\nTanh : (4, 200)\nLinear : (4, 27)\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    logits = model(Xb)\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n    break\n\n      0/ 200000: 3.2837",
    "crumbs": [
      "Makemore Part 5"
    ]
  },
  {
    "objectID": "cnn1.html#choose-channels",
    "href": "cnn1.html#choose-channels",
    "title": "Makemore Part 5",
    "section": "Choose channels",
    "text": "Choose channels\nin such a manner such the overall number of parameters (capacity) of the network is comparable to our naive scale up. We want to see if our new architecture is using this capacity more efficiently.\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 68 # the number of neurons in the hidden layer of the MLP\n\nmodel = Sequential([Embedding(vocab_size, n_embd),\n                    FlattenConsecutive(2),Linear(n_embd*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n                    FlattenConsecutive(2),Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n                    FlattenConsecutive(2),Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),                    \n                    Linear(n_hidden, vocab_size)\n                    ]\n                   )\n\n# parameter init\nwith torch.no_grad():\n    model.layers[-1].weight *= 0.1 # last layer make less confident at initialization\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n22397\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    logits = model(Xb)\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n#     break\n\n      0/ 200000: 3.3072\n  10000/ 200000: 2.5426\n  20000/ 200000: 1.9079\n  30000/ 200000: 2.1577\n  40000/ 200000: 2.2090\n  50000/ 200000: 2.1837\n  60000/ 200000: 1.8979\n  70000/ 200000: 2.4444\n  80000/ 200000: 2.0945\n  90000/ 200000: 1.9290\n 100000/ 200000: 2.4816\n 110000/ 200000: 1.8815\n 120000/ 200000: 1.7618\n 130000/ 200000: 1.8076\n 140000/ 200000: 2.0070\n 150000/ 200000: 2.0590\n 160000/ 200000: 2.0819\n 170000/ 200000: 2.3477\n 180000/ 200000: 2.4015\n 190000/ 200000: 2.2622\n\n\n\n# model.layers[3].running_mean.shape\n\n\nfor layer in model.layers:\n    print(layer.__class__.__name__,':',tuple(layer.out.shape))\n\nEmbedding : (32, 8, 10)\nFlattenConsecutive : (32, 4, 20)\nLinear : (32, 4, 68)\nBatchNorm1d : (32, 4, 68)\nTanh : (32, 4, 68)\nFlattenConsecutive : (32, 2, 136)\nLinear : (32, 2, 68)\nBatchNorm1d : (32, 2, 68)\nTanh : (32, 2, 68)\nFlattenConsecutive : (32, 136)\nLinear : (32, 68)\nBatchNorm1d : (32, 68)\nTanh : (32, 68)\nLinear : (32, 27)\n\n\n\nplt.plot(torch.tensor(lossi).view(-1,1000).mean(1))\n\n\n\n\n\n\n\n\n\n# put layers into eval mode (needed for batchnorm especially)\nfor layer in model.layers:\n    layer.training = False\n\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    logits = model(x)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 1.9392633438110352\nval 2.02504563331604\n\n\n\nfor _ in range(20):\n    out=[]\n    context = [0]*block_size\n    while True:\n        logits = model(torch.tensor([context]))\n        probs = F.softmax(logits, dim=1)\n        \n        ix = torch.multinomial(probs, num_samples=1).item()\n        context = context[1:]+[ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    print(''.join([itos[i] for i in out]))\n\nmchore.\ngaran.\nta.\nramsam.\naugan.\ndaclin.\nmiran.\nadisha.\nisa.\ngailette.\ntaliya.\nvayton.\nemianna.\npapri.\nreis.\nyapi.\nismayani.\ndessey.\nevianich.\nhaddie.",
    "crumbs": [
      "Makemore Part 5"
    ]
  },
  {
    "objectID": "cnn1.html#bug-in-batchnorm",
    "href": "cnn1.html#bug-in-batchnorm",
    "title": "Makemore Part 5",
    "section": "Bug in BatchNorm",
    "text": "Bug in BatchNorm\nIt’s taking the mean of the first dimension this is not what we want now since we have three dimensional items going in.\n\ne = torch.randn(32,4,68)\nemean = e.mean(0,keepdim=True)\nevar = e.var(0,keepdim=True)\nehat = (e - emean)/torch.sqrt(evar + 1e-5)\nprint(emean.shape, evar.shape, ehat.shape)\n\ntorch.Size([1, 4, 68]) torch.Size([1, 4, 68]) torch.Size([32, 4, 68])\n\n\nWhat we want is just 68 numbers for the mean and variance ( one value of mean and one for the variance for each channel). Thus our stats are only coming from 32 numbers whereas we want them to come from 32 times 4 numbers.\nWith broadcasting the shapes work out but the effect is not what we want. So we need to do like so:\n\nemean = e.mean((0,1),keepdim=True)\nevar = e.var((0,1),keepdim=True)\nehat = (e - emean)/torch.sqrt(evar + 1e-5)\nprint(emean.shape, evar.shape, ehat.shape)\n\ntorch.Size([1, 1, 68]) torch.Size([1, 1, 68]) torch.Size([32, 4, 68])\n\n\n\nclass BatchNorm1d:\n  \n    def __init__(self, dim, eps=1e-5, momentum=0.1):\n        self.eps = eps\n        self.momentum = momentum\n        self.training = True\n        # parameters (trained with backprop)\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n        # buffers (trained with a running 'momentum update')\n        self.running_mean = torch.zeros(dim)\n        self.running_var = torch.ones(dim)\n  \n    def __call__(self, x):\n        # calculate the forward pass\n        if self.training:\n            if x.ndim == 2:\n                dim = 0\n            elif x.ndim == 3:\n                dim = (0,1)\n            xmean = x.mean(dim, keepdim=True) # batch mean\n            xvar = x.var(dim, keepdim=True) # batch variance\n        else:\n            xmean = self.running_mean\n            xvar = self.running_var\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n        self.out = self.gamma * xhat + self.beta\n    \n        # update the buffers\n        if self.training:\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n        return self.out\n  \n    def parameters(self):\n        return [self.gamma, self.beta]\n\nWe depart from the API of PyTorch for Batchnorm which expects inputs as either (N,C) or (N,C,L) for us it will be (N,C) or (N,L,C).\nN is batchsize, C= number of features or channels, L is sequence length.\n\nn_embd = 10 # the dimensionality of the character embedding vectors\nn_hidden = 68 # the number of neurons in the hidden layer of the MLP\n\nmodel = Sequential([Embedding(vocab_size, n_embd),\n                    FlattenConsecutive(2),Linear(n_embd*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n                    FlattenConsecutive(2),Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n                    FlattenConsecutive(2),Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),                    \n                    Linear(n_hidden, vocab_size)\n                    ]\n                   )\n\n# parameter init\nwith torch.no_grad():\n    model.layers[-1].weight *= 0.1 # last layer make less confident at initialization\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n22397\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    logits = model(Xb)\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n#     break\n\n      0/ 200000: 3.3042\n  10000/ 200000: 1.8714\n  20000/ 200000: 2.2080\n  30000/ 200000: 2.4897\n  40000/ 200000: 1.9405\n  50000/ 200000: 1.7345\n  60000/ 200000: 2.2309\n  70000/ 200000: 2.4927\n  80000/ 200000: 2.2003\n  90000/ 200000: 1.5394\n 100000/ 200000: 2.2238\n 110000/ 200000: 1.7608\n 120000/ 200000: 1.9861\n 130000/ 200000: 2.1454\n 140000/ 200000: 1.9057\n 150000/ 200000: 2.0836\n 160000/ 200000: 1.8067\n 170000/ 200000: 1.7575\n 180000/ 200000: 1.9857\n 190000/ 200000: 1.6863\n\n\n\nfor layer in model.layers:\n    print(layer.__class__.__name__,':',tuple(layer.out.shape))\n\nEmbedding : (32, 8, 10)\nFlattenConsecutive : (32, 4, 20)\nLinear : (32, 4, 68)\nBatchNorm1d : (32, 4, 68)\nTanh : (32, 4, 68)\nFlattenConsecutive : (32, 2, 136)\nLinear : (32, 2, 68)\nBatchNorm1d : (32, 2, 68)\nTanh : (32, 2, 68)\nFlattenConsecutive : (32, 136)\nLinear : (32, 68)\nBatchNorm1d : (32, 68)\nTanh : (32, 68)\nLinear : (32, 27)\n\n\n\nplt.plot(torch.tensor(lossi).view(-1,1000).mean(1))\n\n\n\n\n\n\n\n\n\n# put layers into eval mode (needed for batchnorm especially)\nfor layer in model.layers:\n    layer.training = False\n\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    logits = model(x)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 1.917731523513794\nval 2.0245659351348877\n\n\n\nfor _ in range(20):\n    out=[]\n    context = [0]*block_size\n    while True:\n        logits = model(torch.tensor([context]))\n        probs = F.softmax(logits, dim=1)\n        \n        ix = torch.multinomial(probs, num_samples=1).item()\n        context = context[1:]+[ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    print(''.join([itos[i] for i in out]))\n\nericopal.\naiden.\njaianna.\nkanah.\nanalimio.\nadhia.\nelanly.\ngengi.\nkaisham.\nroselyd.\nyuri.\nnachellee.\nnorayany.\nbosti.\nsalvi.\nkeerza.\nkadi.\nbrigg.\nkeyonnord.\nmattit.",
    "crumbs": [
      "Makemore Part 5"
    ]
  },
  {
    "objectID": "cnn1.html#scaling-up-wavenet",
    "href": "cnn1.html#scaling-up-wavenet",
    "title": "Makemore Part 5",
    "section": "Scaling up Wavenet",
    "text": "Scaling up Wavenet\n\nn_embd = 24 # the dimensionality of the character embedding vectors\nn_hidden = 128 # the number of neurons in the hidden layer of the MLP\n\nmodel = Sequential([Embedding(vocab_size, n_embd),\n                    FlattenConsecutive(2),Linear(n_embd*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n                    FlattenConsecutive(2),Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n                    FlattenConsecutive(2),Linear(n_hidden*2, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),                    \n                    Linear(n_hidden, vocab_size)\n                    ]\n                   )\n\n# parameter init\nwith torch.no_grad():\n    model.layers[-1].weight *= 0.1 # last layer make less confident at initialization\n\nparameters = model.parameters()\nprint(sum(p.nelement() for p in parameters)) # number of parameters in total\nfor p in parameters:\n    p.requires_grad = True\n\n76579\n\n\n\n# same optimization as last time\nmax_steps = 200000\nbatch_size = 32\nlossi = []\n\nfor i in range(max_steps):\n  \n    # minibatch construct\n    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n\n    # forward pass\n    logits = model(Xb)\n    loss = F.cross_entropy(logits, Yb) # loss function\n\n    # backward pass\n    for p in parameters:\n        p.grad = None\n    loss.backward()\n\n    # update\n    lr = 0.1 if i &lt; 150000 else 0.01 # step learning rate decay\n    for p in parameters:\n        p.data += -lr * p.grad\n\n    # track stats\n    if i % 10000 == 0: # print every once in a while\n        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n    lossi.append(loss.log10().item())\n    \n#     break\n\n      0/ 200000: 3.2992\n  10000/ 200000: 1.8959\n  20000/ 200000: 2.0079\n  30000/ 200000: 1.7454\n  40000/ 200000: 2.4847\n  50000/ 200000: 1.8896\n  60000/ 200000: 1.8285\n  70000/ 200000: 1.3920\n  80000/ 200000: 2.2206\n  90000/ 200000: 1.7741\n 100000/ 200000: 1.8223\n 110000/ 200000: 1.8102\n 120000/ 200000: 1.8542\n 130000/ 200000: 1.8778\n 140000/ 200000: 1.5735\n 150000/ 200000: 1.6645\n 160000/ 200000: 2.0889\n 170000/ 200000: 2.3077\n 180000/ 200000: 1.8694\n 190000/ 200000: 1.8156\n\n\n\nfor layer in model.layers:\n    print(layer.__class__.__name__,':',tuple(layer.out.shape))\n\nEmbedding : (32, 8, 24)\nFlattenConsecutive : (32, 4, 48)\nLinear : (32, 4, 128)\nBatchNorm1d : (32, 4, 128)\nTanh : (32, 4, 128)\nFlattenConsecutive : (32, 2, 256)\nLinear : (32, 2, 128)\nBatchNorm1d : (32, 2, 128)\nTanh : (32, 2, 128)\nFlattenConsecutive : (32, 256)\nLinear : (32, 128)\nBatchNorm1d : (32, 128)\nTanh : (32, 128)\nLinear : (32, 27)\n\n\n\n# put layers into eval mode (needed for batchnorm especially)\nfor layer in model.layers:\n    layer.training = False\n\n\n@torch.no_grad() # this decorator disables gradient tracking\ndef split_loss(split):\n    x,y = {\n    'train': (Xtr, Ytr),\n    'val': (Xdev, Ydev),\n    'test': (Xte, Yte),\n    }[split]\n    logits = model(x)\n    loss = F.cross_entropy(logits, y)\n    print(split, loss.item())\n\nsplit_loss('train')\nsplit_loss('val')\n\ntrain 1.7691593170166016\nval 1.994916319847107\n\n\n\nfor _ in range(20):\n    out=[]\n    context = [0]*block_size\n    while True:\n        logits = model(torch.tensor([context]))\n        probs = F.softmax(logits, dim=1)\n        \n        ix = torch.multinomial(probs, num_samples=1).item()\n        context = context[1:]+[ix]\n        out.append(ix)\n        if ix == 0:\n            break\n    print(''.join([itos[i] for i in out]))\n\nelijah.\ndawous.\nkimo.\nemmy.\njeshun.\nlynn.\nuller.\nmelahni.\nshinhaan.\nhalsan.\nraslyn.\nkent.\njaighna.\nneila.\njacinthyon.\nbez.\nanaiah.\ndray.\nayla.\nmaesan.",
    "crumbs": [
      "Makemore Part 5"
    ]
  },
  {
    "objectID": "cnn1.html#experimental-harness",
    "href": "cnn1.html#experimental-harness",
    "title": "Makemore Part 5",
    "section": "Experimental harness",
    "text": "Experimental harness\nThere is a bump in performance (we cross over the 2 threshold) but now experiments are taking longer. We are in the darkness about the correct setting of the hyperparameters (learning rate etc).\n\nNext time:\nWhy convolutions? Brief preview/hint\n\nfor x,y in zip(Xtr[7:15], Ytr[7:15]):\n    print(''.join(itos[ix.item()] for ix in x), '--&gt;', itos[y.item()])\n\n........ --&gt; d\n.......d --&gt; i\n......di --&gt; o\n.....dio --&gt; n\n....dion --&gt; d\n...diond --&gt; r\n..diondr --&gt; e\n.diondre --&gt; .\n\n\n\n# forward a single example:\nlogits = model(Xtr[[7]])\nlogits.shape\n\ntorch.Size([1, 27])\n\n\n\n# forward all of them\nlogits = torch.zeros(8, 27)\nfor i in range(8):\n    logits[i] = model(Xtr[[7+i]])\nlogits.shape\n\ntorch.Size([8, 27])\n\n\n\n# convolution is a \"for loop\"\n# allows us to forward Linear layers efficiently over space\n\n\n# Fin: 56m21s",
    "crumbs": [
      "Makemore Part 5"
    ]
  }
]